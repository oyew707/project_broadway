{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0rypzv3B0wK"
   },
   "source": [
    "From Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1732131326268,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     },
     "user_tz": 300
    },
    "id": "P-rwaHCLA9XQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial import ConvexHull\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732131326613,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     },
     "user_tz": 300
    },
    "id": "v762KqxEF1H-",
    "outputId": "ee42ad9d-8fa8-4bcd-afa4-55d8ff154180"
   },
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "torch.manual_seed(4)\n",
    "torch.cuda.manual_seed(4)\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732131326613,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     },
     "user_tz": 300
    },
    "id": "JlOSHIYOBMSt"
   },
   "outputs": [],
   "source": [
    "class MyNetworkDataset(Dataset):\n",
    "  def __init__(self, nodes_df, edges_df, num_negative_samples=5):\n",
    "    self.nodes = nodes_df\n",
    "    self.edges = edges_df\n",
    "    self.num_negative_samples = num_negative_samples\n",
    "\n",
    "    # Create node mapping\n",
    "    self.node_to_idx = {node_id: idx for idx, node_id in enumerate(self.nodes['node_id'].unique())}\n",
    "    self.num_nodes = len(self.node_to_idx)\n",
    "\n",
    "    # Create positive edge list\n",
    "    self.positive_edges = []\n",
    "    for _, row in edges_df.iterrows():\n",
    "        buyer_idx = self.node_to_idx[row['buyer_id']]\n",
    "        sp1_idx = self.node_to_idx[row['sponsor1_id']]\n",
    "        sp2_idx = self.node_to_idx[row['sponsor2_id']]\n",
    "        self.positive_edges.extend([(buyer_idx, sp1_idx), (buyer_idx, sp2_idx)])\n",
    "\n",
    "    # Convert to set for faster lookup\n",
    "    self.positive_edges_set = set((i, j) for i, j in self.positive_edges)\n",
    "    self.positive_edges = torch.tensor(self.positive_edges, device=device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.positive_edges)\n",
    "\n",
    "  def generate_negative_edge(self):\n",
    "    while True:\n",
    "      # Randomly sample two nodes\n",
    "      node1 = np.random.randint(0, self.num_nodes)\n",
    "      node2 = np.random.randint(0, self.num_nodes)\n",
    "\n",
    "      # Check if this is not a positive edge and nodes are different\n",
    "      if node1 != node2 and (node1, node2) not in self.positive_edges_set:\n",
    "          return (node1, node2)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Get positive edge\n",
    "    pos_edge = self.positive_edges[idx]\n",
    "\n",
    "    # Generate negative edges\n",
    "    neg_edges = [self.generate_negative_edge() for _ in range(self.num_negative_samples)]\n",
    "    neg_edges = torch.tensor(neg_edges, device=device)\n",
    "\n",
    "    return pos_edge, neg_edges\n",
    "\n",
    "class MyNetworkDatasetII(MyNetworkDataset):\n",
    "  \"\"\"\n",
    "  Random walk sampling of negative edges\n",
    "  \"\"\"\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "  def generate_negative_edge(self):\n",
    "    # Sometimes sample completely random\n",
    "    if np.random.random() < 0.5:\n",
    "        return super().generate_negative_edge()\n",
    "\n",
    "    # Sometimes do a random walk from a positive edge node\n",
    "    pos_edge = self.positive_edges[np.random.randint(len(self.positive_edges))]\n",
    "    start_node = pos_edge[np.random.randint(2)]  # Pick one end of the edge\n",
    "    current_node = start_node\n",
    "\n",
    "    # Take a few random steps\n",
    "    for _ in range(np.random.randint(2, 5)):\n",
    "        # Get all nodes this one is connected to\n",
    "        neighbors = [e[1] for e in self.positive_edges_set if e[0] == current_node] + \\\n",
    "                   [e[0] for e in self.positive_edges_set if e[1] == current_node]\n",
    "\n",
    "        if neighbors:\n",
    "            current_node = np.random.choice(neighbors)\n",
    "\n",
    "    # Return edge between start and end of walk if it's not a positive edge\n",
    "    if start_node != current_node and (start_node, current_node) not in self.positive_edges_set:\n",
    "        return (start_node, current_node)\n",
    "\n",
    "    # Fall back to random sampling if walk didn't work\n",
    "    return super().generate_negative_edge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732131326613,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     },
     "user_tz": 300
    },
    "id": "3iyuXlMFCFWC"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class NodeEmbeddingAutoencoder(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Initialize embeddings with Xavier/Glorot initialization\n",
    "        self.num_nodes = num_nodes\n",
    "        self.node_embeddings = nn.Parameter(torch.randn(num_nodes, embedding_dim, device=device) / np.sqrt(embedding_dim))\n",
    "        self.gradient_counts = torch.zeros(num_nodes)  # Track gradient updates per node\n",
    "\n",
    "    def forward(self, pos_edge, neg_edges):\n",
    "        # Get embeddings for positive edge\n",
    "        pos_node1_embed = self.node_embeddings[pos_edge[0]]\n",
    "        pos_node2_embed = self.node_embeddings[pos_edge[1]]\n",
    "        pos_dist = torch.norm(pos_node1_embed - pos_node2_embed)\n",
    "\n",
    "        # Get embeddings for negative edges\n",
    "        neg_node1_embed = self.node_embeddings[neg_edges[:, 0]]\n",
    "        neg_node2_embed = self.node_embeddings[neg_edges[:, 1]]\n",
    "        neg_dist = torch.norm(neg_node1_embed - neg_node2_embed, dim=1)\n",
    "\n",
    "        # Track which nodes got gradients in this batch\n",
    "        with torch.no_grad():\n",
    "            self.gradient_counts[pos_edge[0].item()] += 1\n",
    "            self.gradient_counts[pos_edge[1].item()] += 1\n",
    "            self.gradient_counts[neg_edges[:, 0].numpy()] += 1\n",
    "            self.gradient_counts[neg_edges[:, 1].numpy()] += 1\n",
    "\n",
    "        return pos_dist, neg_dist\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.node_embeddings.detach().cpu()\n",
    "\n",
    "    def get_gradient_stats(self):\n",
    "        return self.gradient_counts\n",
    "\n",
    "\n",
    "class NodeEmbeddingAutoencoderII(NodeEmbeddingAutoencoder):\n",
    "  \"\"\"\n",
    "  pull rarely updated nodes toward their ethnic group center\n",
    "  \"\"\"\n",
    "  def __init__(self, embedding_dim, dataset, spread_factor=0.2):\n",
    "    super().__init__(dataset.num_nodes, embedding_dim)\n",
    "\n",
    "    # First, initialize ethnicity centers\n",
    "    nodes_df['ethnicity'].fillna('Nan', inplace=True)\n",
    "    unique_ethnicities = nodes_df['ethnicity'].fillna('Nan').unique()\n",
    "    num_ethnicities = len(unique_ethnicities)\n",
    "    ethnicity_centers = defaultdict(list, {k:[] for k in unique_ethnicities})\n",
    "\n",
    "    # init\n",
    "    self.nodes = dataset.nodes\n",
    "    self.ethnicity_centers = ethnicity_centers\n",
    "\n",
    "  def get_ethnicity_centers(self):\n",
    "      # Dynamically update ethnicity centers based on current embeddings\n",
    "      current_centers = {}\n",
    "      for ethnicity in self.ethnicity_centers.keys():\n",
    "          eth_mask = self.nodes['ethnicity'] == ethnicity\n",
    "          eth_embeddings = self.node_embeddings[eth_mask]\n",
    "          current_centers[ethnicity] = eth_embeddings.mean(dim=0)\n",
    "      return current_centers\n",
    "\n",
    "  def get_regularization_loss(self, update_threshold):\n",
    "      # Identify rarely updated nodes\n",
    "      rarely_updated = self.gradient_counts < self.gradient_counts.mean() * update_threshold\n",
    "\n",
    "      if not rarely_updated.any():\n",
    "          return 0.0\n",
    "\n",
    "      # Get current ethnic centers\n",
    "      current_centers = self.get_ethnicity_centers()\n",
    "\n",
    "      # Calculate regularization loss for rarely updated nodes\n",
    "      reg_loss = 0\n",
    "      for idx in torch.where(rarely_updated)[0]:\n",
    "          ethnicity = self.nodes.iloc[int(idx)]['ethnicity']\n",
    "          center = current_centers[ethnicity]\n",
    "          reg_loss += torch.norm(self.node_embeddings[idx] - center)\n",
    "\n",
    "      return reg_loss / rarely_updated.sum()  # Average over rare nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1732131326865,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     },
     "user_tz": 300
    },
    "id": "CLNjg0xyCImB"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "nodes_df = pd.read_csv('nyse_node_sp1.csv',\n",
    "                       names=['name', 'committee', 'node_id', 'ethnicity', 'sponsor'])\n",
    "edges_df = pd.read_csv('nyse_edge_buy_sp_sp1.csv',\n",
    "                       names=['buyer_id', 'sponsor1_id', 'sponsor2_id', 'f1', 'f2', 'f3', 'f4',\n",
    "                             'blackballs', 'whiteballs', 'year'])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MyNetworkDataset(nodes_df, edges_df, num_negative_samples=25)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1732131326865,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     },
     "user_tz": 300
    },
    "id": "yiHttqdhC5X0",
    "outputId": "a8c03cd8-4444-45b1-adb4-86f295f3ebf3"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "EMBEDDING_DIM = 2\n",
    "REG_WEIGHT = 0.1  # Weight for regularization term\n",
    "UPDATE_THRESHOLD = 0.1  # Threshold for considering a node rarely updated\n",
    "model = NodeEmbeddingAutoencoderII(EMBEDDING_DIM, dataset)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOnOSQuwDaRi"
   },
   "source": [
    "Note: we use contrastive loss to help keep connected nodes closer in the embedding space and push things away when not connected\n",
    "\n",
    "See[link](https://lilianweng.github.io/posts/2021-05-31-contrastive/#contrastive-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732131326865,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     },
     "user_tz": 300
    },
    "id": "e4dasHznDZ5A"
   },
   "outputs": [],
   "source": [
    "# Margin for contrastive loss\n",
    "MARGIN = 1.0\n",
    "# Training\n",
    "NUM_EPOCHS = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDHTFJmtDuft",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732133589480,
     "user_tz": 300,
     "elapsed": 2262620,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     }
    },
    "outputId": "cbf81eaf-fa3b-44aa-ca15-c0bc0d09fc02"
   },
   "outputs": [],
   "source": [
    "losses, regularization = [], []\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    total_loss, total_reg = 0, 0\n",
    "    for batch_pos_edges, batch_neg_edges in dataloader:\n",
    "        # Forward pass for each edge in batch\n",
    "        batch_loss, batch_reg = 0, 0\n",
    "        for pos_edge, neg_edges in zip(batch_pos_edges, batch_neg_edges):\n",
    "            pos_dist, neg_dist = model(pos_edge, neg_edges)\n",
    "\n",
    "            # Contrastive loss: minimize positive distances, maximize negative distances up to margin\n",
    "            # Regularization loss pushes nodes that have not been updated towards their cluster mean\n",
    "            contrastive_loss = pos_dist + torch.mean(torch.clamp(MARGIN - neg_dist, min=0))\n",
    "            if epoch > 20:\n",
    "                reg_loss = model.get_regularization_loss(update_threshold=UPDATE_THRESHOLD)\n",
    "            else:\n",
    "                reg_loss = 0\n",
    "            loss = contrastive_loss + REG_WEIGHT * reg_loss\n",
    "            batch_loss += loss\n",
    "            batch_reg += reg_loss\n",
    "\n",
    "        batch_loss = batch_loss / len(batch_pos_edges)\n",
    "        batch_reg = batch_reg / len(batch_pos_edges)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss.item()\n",
    "        total_reg += batch_reg\n",
    "\n",
    "    losses.append(total_loss/len(dataloader))\n",
    "    regularization.append(total_reg/len(dataloader))\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Save Embeddings**"
   ],
   "metadata": {
    "id": "3jZjA9hZuX9X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "node_embeddings = model.get_embeddings().numpy()\n",
    "tensorflow_tensor = tf.convert_to_tensor(node_embeddings)\n",
    "np.save('node_embeddings.npy', node_embeddings)"
   ],
   "metadata": {
    "id": "zR5u7OXxuXEb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732133766065,
     "user_tz": 300,
     "elapsed": 333,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhOFMg_yEum-"
   },
   "source": [
    "**Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "NQH5LRxrknmj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732133589836,
     "user_tz": 300,
     "elapsed": 362,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     }
    },
    "outputId": "aaefe4e0-d810-4290-e6ea-76fc6b623a7f"
   },
   "outputs": [],
   "source": [
    "# After training, analyze gradient distribution:\n",
    "gradient_counts = model.get_gradient_stats()\n",
    "print(\"Gradient update statistics:\")\n",
    "print(f\"Mean updates per node: {gradient_counts.mean():.2f}\")\n",
    "print(f\"Min updates per node: {gradient_counts.min():.2f}\")\n",
    "print(f\"Max updates per node: {gradient_counts.max():.2f}\")\n",
    "\n",
    "# Visualize gradient distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(gradient_counts, bins=50)\n",
    "plt.title('Distribution of Gradient Updates Across Nodes')\n",
    "plt.xlabel('Number of Updates')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "DpwgdvNWmZEr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732133590170,
     "user_tz": 300,
     "elapsed": 338,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     }
    },
    "outputId": "213968bc-8a46-41a7-8f70-c137b5709252"
   },
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "sns.lineplot(x=list(range(len(losses))), y=losses, label='Contrastive loss')\n",
    "sns.lineplot(x=list(range(len(regularization))), y=regularization, label='regularization')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFevmzQbEXSQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732133590170,
     "user_tz": 300,
     "elapsed": 5,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get final embeddings\n",
    "SUBS= 2000\n",
    "node_embeddings = model.get_embeddings().numpy()[:SUBS]\n",
    "full_node_df = nodes_df.copy()\n",
    "nodes_df = nodes_df.iloc[:SUBS]\n",
    "# full_edges_df = edges_df.copy()\n",
    "# edges_df = edges_df.iloc[:SUBS]\n",
    "# full_dataset = dataset\n",
    "# dataset = Subset(full_dataset, range(SUBS)).dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "id": "T1-ZwYmSEuWB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732133592847,
     "user_tz": 300,
     "elapsed": 2681,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     }
    },
    "outputId": "52b13a6d-b592-4b5d-860b-a2988d9530db"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot nodes\n",
    "colors = nodes_df['ethnicity'].astype('category').cat.codes\n",
    "scatter = plt.scatter(node_embeddings[:, 0], node_embeddings[:, 1],\n",
    "                     c=colors, alpha=0.6, cmap='tab10')\n",
    "\n",
    "# Plot positive edges\n",
    "for edge in dataset.positive_edges:\n",
    "    node1, node2 = edge\n",
    "    if node1 >= SUBS or node2 >= SUBS:\n",
    "        continue\n",
    "    plt.plot([node_embeddings[node1, 0], node_embeddings[node2, 0]],\n",
    "             [node_embeddings[node1, 1], node_embeddings[node2, 1]],\n",
    "             'gray', alpha=0.1)\n",
    "\n",
    "# Add legend\n",
    "legend1 = plt.legend(*scatter.legend_elements(),\n",
    "                    title=\"Ethnicity Groups\",\n",
    "                    loc=\"upper right\")\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.title('NYSE Network Node Embeddings\\nConnected nodes are closer, unconnected nodes are farther')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "ne = model.get_embeddings().numpy()\n",
    "# Plot nodes\n",
    "colors = full_node_df['ethnicity'].astype('category').cat.codes\n",
    "scatter = plt.scatter(ne[:, 0], ne[:, 1],\n",
    "                     c=colors, alpha=0.6, cmap='tab10')\n",
    "\n",
    "# Plot positive edges\n",
    "for edge in dataset.positive_edges:\n",
    "    node1, node2 = edge\n",
    "    plt.plot([ne[node1, 0], ne[node2, 0]],\n",
    "             [ne[node1, 1], ne[node2, 1]],\n",
    "             'gray', alpha=0.1)\n",
    "\n",
    "# Add legend\n",
    "legend1 = plt.legend(*scatter.legend_elements(),\n",
    "                    title=\"Ethnicity Groups\",\n",
    "                    loc=\"upper right\")\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "plt.title('NYSE Network Node Embeddings\\nConnected nodes are closer, unconnected nodes are farther')\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "id": "QkqVP5b8QOnX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732134430904,
     "user_tz": 300,
     "elapsed": 15455,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     }
    },
    "outputId": "df63d177-afab-41b7-f913-f04ef88b6e2e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kIgBwCf_MmPV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732133596129,
     "user_tz": 300,
     "elapsed": 3286,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     }
    },
    "outputId": "b7c16185-18ef-4eee-b219-568e36bf2482"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_ethnic_clusters(node_embeddings, nodes_df, title, sample_edges=1000):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plot by ethnicity with different subplots\n",
    "    unique_ethnicities = nodes_df['ethnicity'].unique()\n",
    "    num_ethnicities = len(unique_ethnicities)\n",
    "    rows = (num_ethnicities + 2) // 3  # Ceiling division for number of rows\n",
    "\n",
    "    for i, ethnicity in enumerate(unique_ethnicities, 1):\n",
    "        plt.subplot(rows, 3, i)\n",
    "\n",
    "        # Get indices for this ethnicity\n",
    "        eth_mask = nodes_df['ethnicity'] == ethnicity\n",
    "        eth_indices = nodes_df[eth_mask].index\n",
    "\n",
    "        # Plot all nodes as light gray background\n",
    "        plt.scatter(node_embeddings[:, 0], node_embeddings[:, 1],\n",
    "                   c='lightgray', alpha=0.1, s=1)\n",
    "\n",
    "        # Plot this ethnicity's nodes\n",
    "        eth_points = node_embeddings[eth_indices]\n",
    "        plt.scatter(eth_points[:, 0], eth_points[:, 1],\n",
    "                   alpha=0.6, label=ethnicity)\n",
    "\n",
    "        # Optional: Draw convex hull around the ethnic group\n",
    "        if len(eth_points) > 3:  # Need at least 3 points for convex hull\n",
    "            hull = ConvexHull(eth_points)\n",
    "            for simplex in hull.simplices:\n",
    "                plt.plot(eth_points[simplex, 0], eth_points[simplex, 1], 'k-', alpha=0.3)\n",
    "\n",
    "        plt.title(f'{ethnicity} (n={len(eth_indices)})')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "def plot_degree_distribution(node_embeddings, dataset, nodes_df):\n",
    "    # Calculate node degrees\n",
    "    degrees = np.zeros(len(nodes_df))\n",
    "    for edge in dataset.positive_edges.cpu():\n",
    "        node1, node2 = edge\n",
    "        if node1 >= SUBS or node2 >= SUBS:\n",
    "            continue\n",
    "        degrees[edge[0]] += 1\n",
    "        degrees[edge[1]] += 1\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Degree distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(degrees, bins=50)\n",
    "    plt.title('Node Degree Distribution')\n",
    "    plt.xlabel('Degree')\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "    # Degree vs. position\n",
    "    plt.subplot(1, 2, 2)\n",
    "    scatter = plt.scatter(node_embeddings[:, 0], node_embeddings[:, 1],\n",
    "                         c=degrees, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter, label='Node Degree')\n",
    "    plt.title('Node Positions Colored by Degree')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_temporal_evolution(node_embeddings, edges_df, nodes_df, num_periods=4):\n",
    "    # Get year range\n",
    "    years = edges_df['year'].sort_values().unique()\n",
    "    year_splits = np.array_split(years, num_periods)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    for i, year_group in enumerate(year_splits, 1):\n",
    "        plt.subplot(1, num_periods, i)\n",
    "\n",
    "        # Get active nodes in this period\n",
    "        period_edges = edges_df[edges_df['year'].isin(year_group)]\n",
    "        active_nodes = set()\n",
    "        for col in ['buyer_id', 'sponsor1_id', 'sponsor2_id']:\n",
    "            active_nodes.update(period_edges[col].unique())\n",
    "\n",
    "        # Convert node IDs to indices\n",
    "        active_indices = [dataset.node_to_idx[node_id] for node_id in active_nodes]\n",
    "\n",
    "        # Plot all nodes as background\n",
    "        plt.scatter(node_embeddings[:, 0], node_embeddings[:, 1],\n",
    "                   c='lightgray', alpha=0.1, s=1)\n",
    "\n",
    "        # Plot active nodes colored by ethnicity\n",
    "        active_ethnicities = nodes_df.loc[nodes_df['node_id'].isin(active_nodes), 'ethnicity']\n",
    "        scatter = plt.scatter(node_embeddings[active_indices, 0],\n",
    "                            node_embeddings[active_indices, 1],\n",
    "                            c=active_ethnicities.astype('category').cat.codes,\n",
    "                            alpha=0.6)\n",
    "\n",
    "        plt.title(f'Years {year_group[0]}-{year_group[-1]}\\n(n={len(active_nodes)})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_high_degree_neighborhoods(node_embeddings, dataset, nodes_df, top_n=5):\n",
    "    # Calculate node degrees\n",
    "    degrees = np.zeros(len(nodes_df))\n",
    "    for edge in dataset.positive_edges.cpu():\n",
    "        degrees[edge[0]] += 1\n",
    "        degrees[edge[1]] += 1\n",
    "\n",
    "    # Get top nodes by degree\n",
    "    top_indices = np.argsort(degrees)[-top_n:]\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        plt.subplot(1, top_n, i)\n",
    "\n",
    "        # Get node info\n",
    "        node_id = list(dataset.node_to_idx.keys())[list(dataset.node_to_idx.values()).index(idx)]\n",
    "        node_info = nodes_df[nodes_df['node_id'] == node_id].iloc[0]\n",
    "\n",
    "        # Get neighbors\n",
    "        neighbors = []\n",
    "        for edge in dataset.positive_edges.cpu():\n",
    "            if edge[0] == idx:\n",
    "                neighbors.append(edge[1])\n",
    "            elif edge[1] == idx:\n",
    "                neighbors.append(edge[0])\n",
    "\n",
    "        # Plot all nodes as background\n",
    "        plt.scatter(node_embeddings[:, 0], node_embeddings[:, 1],\n",
    "                   c='lightgray', alpha=0.1, s=1)\n",
    "\n",
    "        # Plot neighbors\n",
    "        plt.scatter(node_embeddings[neighbors, 0], node_embeddings[neighbors, 1],\n",
    "                   alpha=0.6, label='Neighbors')\n",
    "\n",
    "        # Plot central node\n",
    "        plt.scatter(node_embeddings[idx, 0], node_embeddings[idx, 1],\n",
    "                   c='red', s=100, label='Center')\n",
    "\n",
    "        plt.title(f\"{node_info['name']}\\n{node_info['ethnicity']}\\nDegree: {int(degrees[idx])}\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create all visualizations\n",
    "plot_ethnic_clusters(node_embeddings, nodes_df, 'Node Embeddings by Ethnicity')\n",
    "plot_degree_distribution(node_embeddings, dataset, nodes_df)\n",
    "# plot_temporal_evolution(node_embeddings, edges_df, nodes_df)\n",
    "# plot_high_degree_neighborhoods(node_embeddings, dataset, nodes_df)\n",
    "\n",
    "# Additional analysis: Calculate and print some statistics\n",
    "print(\"\\nNetwork Statistics:\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "kZq42Sgx455q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1732133596129,
     "user_tz": 300,
     "elapsed": 9,
     "user": {
      "displayName": "Stein Oyewole",
      "userId": "13069745930786112792"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOOlyUjiCsrTB7buz9KN4O6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
