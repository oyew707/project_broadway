{"cells":[{"cell_type":"markdown","source":["# Vectorized Broadway HMCMC"],"metadata":{"collapsed":false,"id":"f6e2353c03adf3e1"},"id":"f6e2353c03adf3e1"},{"cell_type":"code","source":["try:\n","    from google.colab import drive\n","    import os\n","    drive.mount('/content/drive')\n","    os.chdir('drive/MyDrive/School/DS-GA 1006/code')\n","    print(os.getcwd())\n","except:\n","  pass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iwZQ1lnEX6if","executionInfo":{"status":"ok","timestamp":1732161739257,"user_tz":300,"elapsed":911,"user":{"displayName":"Stein Oyewole","userId":"13069745930786112792"}},"outputId":"d1aee665-7640-4b6a-f46e-c38fd843c690"},"id":"iwZQ1lnEX6if","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/School/DS-GA 1006/code\n"]}]},{"cell_type":"code","source":["# ! pip install -r requirements.txt"],"metadata":{"id":"O7e5Ts67X-5L"},"id":"O7e5Ts67X-5L","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### imports"],"metadata":{"collapsed":false,"id":"d34ac4c37e279b7d"},"id":"d34ac4c37e279b7d"},{"cell_type":"code","execution_count":null,"id":"initial_id","metadata":{"collapsed":true,"ExecuteTime":{"end_time":"2024-10-23T04:15:14.782499Z","start_time":"2024-10-23T04:15:10.221285Z"},"id":"initial_id"},"outputs":[],"source":["import tensorflow as tf\n","import pandas as pd\n","import tensorflow_probability as tfp\n","from tqdm import tqdm\n","from functools import lru_cache\n","import numpy as np\n","from collections import defaultdict\n","import copy\n","import gc"]},{"cell_type":"code","source":["tf.config.run_functions_eagerly(True)\n","SEED = 4\n","tf.random.set_seed(SEED)\n","parameter_initializer = tf.keras.initializers.RandomNormal(\n","    seed=SEED\n",")"],"metadata":{"id":"RgL56_z_YrYc"},"id":"RgL56_z_YrYc","execution_count":null,"outputs":[]},{"cell_type":"code","outputs":[],"source":["# Read the data files\n","node_data = pd.read_csv('data/nyse_node_sp1.csv', header=None,\n","                        names=['name', 'ever_committee', 'node_id', 'ethnicity', 'ever_sponsor'])\n","edge_data = pd.read_csv('data/nyse_edge_buy_sp_sp1.csv', header=None,\n","                        names=['buyer_id', 'sponsor1_id', 'sponsor2_id', 'f1', 'f2', 'f3', 'f4', 'blackballs', 'whiteballs', 'year'])\n","committee_data = pd.read_csv('data/nyse_edge_buy_com1.csv', header=None,\n","                             names=['buyer_id', 'committee_id', 'f1', 'f2', 'f3', 'f4', 'blackballs', 'whiteballs', 'year'])"],"metadata":{"ExecuteTime":{"end_time":"2024-10-23T04:15:14.835267Z","start_time":"2024-10-23T04:15:14.781960Z"},"id":"4158787232e6a735"},"id":"4158787232e6a735","execution_count":null},{"cell_type":"code","outputs":[],"source":["def process_data(node_data, edge_data, committee_data):\n","    node_data = pd.get_dummies(data=node_data, columns=['ethnicity'], dummy_na=True, prefix='ethnicity', drop_first=True, dtype=int)\n","    node_data[['ever_committee', 'ever_sponsor']] = node_data[['ever_committee', 'ever_sponsor']].fillna(0)\n","    node_attrs = node_data.set_index('node_id').drop(columns=['name']).T.to_dict('list')\n","\n","    # Initialize network statistics\n","    network_stats = {node_id: {'degree': 0, 'sponsor_count': 0} for node_id in node_attrs}\n","    edges = defaultdict(set)\n","\n","    transactions = []\n","    for _, row in edge_data.iterrows():\n","        buyer_id = row['buyer_id']\n","        sponsor1_id = row['sponsor1_id']\n","        sponsor2_id = row['sponsor2_id']\n","        year = row['year']\n","\n","        # Update network statistics\n","        network_stats[buyer_id]['degree'] += 2\n","        network_stats[sponsor1_id]['degree'] += 1\n","        network_stats[sponsor2_id]['degree'] += 1\n","        network_stats[sponsor1_id]['sponsor_count'] += 1\n","        network_stats[sponsor2_id]['sponsor_count'] += 1\n","        edges[buyer_id].add(sponsor1_id)\n","        edges[buyer_id].add(sponsor2_id)\n","\n","        committee_members = committee_data[(committee_data['buyer_id'] == buyer_id) &\n","                                           (committee_data['year'] == year)]['committee_id'].tolist()\n","\n","        transactions.append({\n","            'buyer_id': buyer_id,\n","            'sponsor1_id': sponsor1_id,\n","            'sponsor2_id': sponsor2_id,\n","            'committee_members': committee_members,\n","            'year': year,\n","            'whiteballs': row['whiteballs'],\n","            'blackballs': row['blackballs']\n","        })\n","\n","    return node_attrs, transactions, network_stats, edges\n","\n","node_attrs, transactions, network_stats, edges = process_data(node_data, edge_data, committee_data)\n","node_stats = {k: v['degree'] for k, v in network_stats.items()}"],"metadata":{"ExecuteTime":{"end_time":"2024-10-23T04:15:15.858189Z","start_time":"2024-10-23T04:15:14.830796Z"},"id":"d4ca0edf3d9be9c4"},"id":"d4ca0edf3d9be9c4","execution_count":null},{"cell_type":"markdown","source":["### Model\n"],"metadata":{"collapsed":false,"id":"e8449435115ace96"},"id":"e8449435115ace96"},{"cell_type":"code","outputs":[],"source":["class VectorizedVStar(tf.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    @tf.function\n","    def U_star(self, xi, xj, si, sj, theta):\n","        \"\"\"\n","        Vectorized utility function that handles tensors of inputs\n","        Args:\n","            xi: [unique_n, 1, x_dim]\n","            xj: [n, 1, x_dim]\n","            si: [unique_n, 1]\n","            sj: [n, 1]\n","            theta: [param_dim] parameters\n","        Return:\n","            U* [unique_n, n] tensor of utility values\n","        \"\"\"\n","        unique_n = tf.shape(xi)[0]\n","        n = tf.shape(xj)[0]\n","\n","        # Reshape theta to [param_dim, 1] for matmul\n","        theta_reshaped = tf.convert_to_tensor(theta, dtype=tf.float16)[:, tf.newaxis]\n","\n","        # Concatenate inputs along the last dimension\n","        if len(xi.shape) == 3: #Input should be [batch_size, x_dim]'\n","            # Reshape si, sj to match broadcasting dimensions\n","            si_expanded = tf.tile(tf.expand_dims(si, -1), [1, n, 1])  # [unique_n, n, 1]\n","            sj_expanded = tf.tile(tf.expand_dims(sj, 0), [unique_n, 1, 1])  # [unique_n, n, 1]\n","            xi_expanded = tf.tile(xi, [1, n, 1])  # [unique_n, n, x_dim]\n","            xj = tf.squeeze(xj, axis=1)  # Remove the middle dimension first: [n, x_dim]\n","            xj = tf.expand_dims(xj, 0)   # Add dimension at start: [1, n, x_dim]\n","            xj_expanded = tf.tile(xj, [unique_n, 1, 1])  # [unique_n, n, x_dim]\n","\n","            # Concatenate features\n","            inputs = tf.concat([\n","                xi_expanded,\n","                xj_expanded,\n","                si_expanded,\n","                sj_expanded\n","            ], axis=-1)  # [unique_n, n, param_dim]\n","            assert len(inputs.shape) == 3 and inputs.shape[-1] == theta.shape[0], f'{inputs.shape=} {theta.shape[0]=}'\n","            inputs = tf.reshape(inputs, [-1, inputs.shape[-1]])  # [unique_n*n, param_dim]\n","\n","            # Perform dot product using matmul: [batchsize, param_dim] @ [param_dim, 1] -> [batchsize, 1]\n","            dot_products = tf.matmul(tf.cast(inputs, tf.float16), theta_reshaped)\n","\n","            # Remove the last dimension and reshape to [unique_n, n]\n","            result = tf.reshape(dot_products, [unique_n, n])\n","        else:\n","            inputs = tf.concat([xi, xj, si[..., tf.newaxis], sj[..., tf.newaxis]], axis=-1)\n","\n","            result = tf.matmul(tf.cast(inputs, tf.float16), theta_reshaped)\n","\n","        return result\n","\n","    @tf.function\n","    def V_star(self, xi, xj, si, sj, theta):\n","        \"\"\"\n","        Vectorized pseudo-surplus function that handles tensors of inputs\n","        \"\"\"\n","        # Compute both directions simultaneously\n","        forward = self.U_star(xi, xj, si, sj, theta)\n","        backward = tf.transpose(self.U_star(xj, xi, sj, si, theta))\n","        return forward + backward\n","\n","    def __call__(self, xi, xj, si, sj, theta):\n","        \"\"\"\n","        Main call method that handles both individual pairs and batches\n","        \"\"\"\n","        # If inputs are already tensors, use them directly\n","        if isinstance(xi, tf.Tensor):\n","            return self.V_star(xi, xj, si, sj, theta)\n","\n","        # Convert individual inputs to tensors if needed\n","        xi = tf.convert_to_tensor(xi, dtype=tf.float16)\n","        xj = tf.convert_to_tensor(xj, dtype=tf.float16)\n","        si = tf.convert_to_tensor(si, dtype=tf.float16)\n","        sj = tf.convert_to_tensor(sj, dtype=tf.float16)\n","        theta = tf.convert_to_tensor(theta, dtype=tf.float16)\n","\n","        return self.V_star(xi, xj, si, sj, theta)\n","\n","class VectorizedPsi(tf.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def __call__(self, H, x_batch, s_batch, w_batch, V_batch, H_batch):\n","        \"\"\"\n","        Vectorized PSI computation for batches of inputs\n","        Args:\n","            H: Function that takes a tensor of x values and returns H values with k dimensions\n","            x_batch: [unique_n, n, x_dim] tensor of x values\n","            s_batch: [unique_n, n] tensor of s values\n","            w_batch: [unique_n, n] tensor of weight values\n","            V_batch: [unique_n, n] tensor of V values\n","            h_values: [unique_n, n, k] tensor of H values\n","        Returns:\n","            [unique_n, n, k] tensor of PSI values\n","        \"\"\"\n","        unique_n, n, x_dim = tf.shape(x_batch)[0], tf.shape(x_batch)[1], tf.shape(x_batch)[2]\n","        k_dim = tf.shape(H_batch)[2]\n","\n","        # Flatten the inputs\n","        x_flat = tf.reshape(x_batch, [-1, x_dim])  # [unique_n*n, x_dim]\n","        s_flat = tf.reshape(s_batch, [-1])         # [unique_n*n]\n","        w_flat = tf.reshape(w_batch, [-1])         # [unique_n*n]\n","        V_flat = tf.reshape(V_batch, [-1])         # [unique_n*n]\n","        H_values = tf.reshape(H_batch, [-1, k_dim])# [unique_n*n, k]\n","\n","        # Compute numerator\n","        num = tf.exp(V_flat)[..., tf.newaxis]  # [unique_n*n, 1]\n","        if tf.reduce_any(tf.math.is_nan(num)) or tf.reduce_any(tf.math.is_inf(num)):\n","            print(f'{V_flat=}')\n","            print(f'{num=}')\n","            raise ValueError('NAN when computing psi: numerator')\n","\n","        # Compute denominator\n","        denom = 1 + H_values  # [unique_n*n, k]\n","\n","        # Expand dimensions for broadcasting\n","        s_flat = s_flat[..., tf.newaxis]  # [unique_n*n, 1]\n","        w_flat = w_flat[..., tf.newaxis]  # [unique_n*n, 1]\n","\n","        # Compute final result\n","        res_flat = w_flat * s_flat * num / denom # [unique_n*n, k]\n","        if tf.reduce_any(tf.math.is_nan(res_flat)) or tf.reduce_any(tf.math.is_inf(res_flat)):\n","            print(f'{w_flat=}')\n","            print(f'{s_flat=}')\n","            print(f'{num=}')\n","            print(f'{H_values=}')\n","            raise ValueError('NAN when computing psi: all results')\n","\n","        # Reshape back to original dimensions\n","        res = tf.reshape(res_flat, [unique_n, n, -1])  # [unique_n, n, k]\n","        return res\n","\n","# Helper class to store H values for vectorized lookup\n","\n","\n","def compute_all_v_values(unique_x, x_tensor, unique_s, s_tensor, theta):\n","    \"\"\"Compute V values for all unique pairs with all nodes at once\"\"\"\n","\n","    x_unique = unique_x[:, tf.newaxis, :] # [unique_n, 1, x_dim]\n","    s_unique = unique_s[:, tf.newaxis]   # [unique_n, 1]\n","    x_all = x_tensor[:, tf.newaxis, :]    # [n, 1, x_dim]\n","    s_all = s_tensor[:, tf.newaxis]    # [n, 1]\n","\n","    return VectorizedVStar()(x_unique, x_all, s_unique, s_all, theta)\n","\n","def compute_psi_values(unique_x, x_tensor, w_tensor, s_tensor, H, V_values):\n","    \"\"\"Compute PSI values for all pairs at once\"\"\"\n","    n_unique = tf.shape(unique_x)[0]\n","    n_nodes = tf.shape(x_tensor)[0]\n","\n","    H_values = H(unique_x)\n","\n","    # Reshape inputs for broadcasting\n","    x_all = tf.tile(x_tensor[tf.newaxis, ...], [n_unique, 1, 1])  # [unique_n, n, x_dim]\n","    s_all = tf.tile(s_tensor[tf.newaxis, ...], [n_unique, 1])     # [unique_n, n]\n","    w_all = tf.tile(w_tensor[tf.newaxis, ...], [n_unique, 1])     # [unique_n, n]\n","    h_all = tf.tile(H_values[tf.newaxis, ...], [1, n_nodes, H.k_dim])  # [unique_n, n, k]\n","\n","    # Reshape V values to match\n","    V_reshaped = tf.reshape(V_values, [n_unique, n_nodes])  # [unique_n, n]\n","\n","    # Compute PSI values for all pairs\n","    res = VectorizedPsi()(H, x_all, s_all, w_all, V_reshaped, h_all)  # [unique_n, n]\n","    return res\n","\n","def H_star(node_attrs, node_stats, weights, theta, H, max_iter=20):\n","\n","    # initialize tensors for vectorized processing\n","    H_current = H\n","    x_tensor = tf.convert_to_tensor(np.array([node_attrs[i] for i in node_attrs.keys()]), dtype=tf.float16)\n","    s_tensor = tf.convert_to_tensor(np.array([node_stats[i] for i in node_attrs.keys()]), dtype=tf.float16)\n","    w_tensor = tf.convert_to_tensor(np.array([weights[i] for i in node_attrs.keys()]), dtype=tf.float16)\n","    unique_node_pairs = set((tuple(node_attrs[node_id]), node_stats[node_id])\n","                          for node_id in node_attrs.keys())\n","    unique_x = tf.convert_to_tensor([x for x, s in unique_node_pairs], dtype=tf.float16)\n","    unique_s = tf.convert_to_tensor([s for x, s in unique_node_pairs], dtype=tf.float16)\n","\n","    x_group = defaultdict(list)\n","    # Grouped indices for each unique x\n","    for idx, (x, _) in enumerate(unique_node_pairs):\n","        x_key = VectorizedH.generate_key(x)\n","        x_group[x_key].append(idx)\n","\n","    V = tf.stop_gradient(compute_all_v_values(unique_x, x_tensor, unique_s, s_tensor, theta))\n","    print(f'Computed {V=} \\n Using {theta=}')\n","\n","    for i in tqdm(range(max_iter), desc='H_star', position=0, leave=True):\n","        H_prev = copy.deepcopy(H_current)\n","\n","        psi_values = tf.stop_gradient(compute_psi_values(unique_x, x_tensor, w_tensor, s_tensor, H_prev, V))\n","        # Compute means for each unique x\n","        psi_mean = {}\n","        for x, indices in x_group.items():\n","            selected_psi_values = tf.gather(psi_values, indices, axis=0)\n","            psi = tf.stop_gradient(tf.reduce_mean(tf.cast(selected_psi_values, tf.float32), axis=[0, 1]))\n","            psi_mean[x] = tf.cast(psi, tf.float16)\n","\n","        # Update H lookup\n","        H_current.update(psi_mean)\n","\n","        # Check convergence - now comparing full tensors\n","        diff = tf.stop_gradient(tf.reduce_max(tf.abs(H_current(unique_x) - H_prev(unique_x))))\n","        if diff < 1e-4:\n","            print(f\"Convergence achieved after {i} iterations.\")\n","            break\n","\n","    return H_current\n","\n","# Defined the importance weight function (section 4.2.1 pg 36)\n","@lru_cache(maxsize=1000)\n","def calculate_weights(s, node_vals):\n","    t = tf.convert_to_tensor(list(node_vals))\n","    indicator_tensor = tf.cast(tf.greater(t, 0), tf.float16)\n","    denominator = tf.reduce_mean(indicator_tensor).numpy()\n","    return s / denominator"],"metadata":{"ExecuteTime":{"end_time":"2024-10-25T18:05:46.302675Z","start_time":"2024-10-25T18:05:46.118829Z"},"id":"9679a87c21a81f1e"},"id":"9679a87c21a81f1e","execution_count":null},{"cell_type":"code","outputs":[],"source":["# Log likelihood contribution for node_id\n","def vectorized_log_likelihood_contribution(Lijt, x_i, x_j, s_i, s_j, theta, H ):\n","    \"\"\"\n","    Args:\n","        Lijt: Tensor of shape [batch_size]\n","        x_i: Tensor of shape [batch_size, feature_dim]\n","        x_j: Tensor of shape [batch_size, feature_dim]\n","        s_i: Tensor of shape [batch_size]\n","        s_j: Tensor of shape [batch_size]\n","        theta: Model parameters\n","        H: H function\n","        v_star: V* function\n","    \"\"\"\n","    # Compute V* for all pairs at once\n","    V = VectorizedVStar()(x_i, x_j, s_i, s_j, theta)\n","\n","    # Compute H values for all nodes at once\n","    H_i = H(x_i)\n","    H_j = H(x_j)\n","\n","    # Compute log likelihood contributions vectorized\n","    ll = 0.5 * Lijt * (V - tf.math.log1p(H_i) - tf.math.log1p(H_j))\n","    return ll\n","\n","\n","def log_likelihood_optimized(theta, node_attrs, node_stats, weights, H = None):\n","    # cast theta to less precision (memory issues and whatnot)\n","    theta = tf.cast(theta, tf.float16)\n","\n","    # # initialize H function and attribute tensors\n","    H = H if H is not None else VectorizedH(k_dim=1, node_attrs=node_attrs, node_stats=node_stats)\n","    H = H_star(node_attrs, node_stats, weights, theta, H)\n","    x_i, x_j, s_i, s_j = [], [], [], []\n","    for i in node_attrs.keys():\n","        for j in edges[i]:\n","            if i == j:\n","                continue\n","            x_i.append(node_attrs[i])\n","            x_j.append(node_attrs[j])\n","            s_i.append(node_stats[i])\n","            s_j.append(node_stats[j])\n","    x_i = tf.convert_to_tensor(x_i, dtype=tf.float16)\n","    x_j = tf.convert_to_tensor(x_j, dtype=tf.float16)\n","    s_i = tf.convert_to_tensor(s_i, dtype=tf.float16)\n","    s_j = tf.convert_to_tensor(s_j, dtype=tf.float16)\n","    Lijt = tf.ones(s_i.shape, dtype=tf.float16)\n","\n","    # compute likelihood for alledges at once\n","    edge_ll = vectorized_log_likelihood_contribution(\n","        Lijt, x_i, x_j, s_i, s_j, theta, H,\n","    )\n","\n","    # Sum edge contributions\n","    # print(f\"Is NA or infinity: {tf.reduce_any(tf.math.is_nan(edge_ll)) or tf.reduce_any(tf.math.is_inf(edge_ll))}\")\n","    ll = tf.reduce_sum(tf.cast(edge_ll, tf.float32)) / tf.cast(tf.reduce_sum(Lijt), tf.float32)\n","    # print(f'Log likelihood: {ll}')\n","\n","\n","    # Add node-specific terms\n","    node_terms = tf.math.log(s_i) - tf.math.log1p(H(x_i))\n","    # Note node_terms should exist because we only consider s_i > 0 and 1 + H > 0\n","    ll += tf.reduce_sum(tf.cast(node_terms, tf.float32)) / tf.cast(tf.shape(x_i)[0], tf.float32)\n","    # return tf.math.reduce_sum(theta), H\n","\n","    return ll, H\n"],"metadata":{"ExecuteTime":{"end_time":"2024-10-23T04:15:15.875179Z","start_time":"2024-10-23T04:15:15.870096Z"},"id":"dcd5dad436895c16"},"id":"dcd5dad436895c16","execution_count":null},{"cell_type":"code","outputs":[{"output_type":"stream","name":"stdout","text":["initial_state=<tf.Variable 'Variable:0' shape=(20,) dtype=float16, numpy=\n","array([-0.0291   ,  0.04324  , -0.0783   ,  0.00989  ,  0.03323  ,\n","       -0.04358  ,  0.01701  ,  0.00419  , -0.06586  , -0.0003023,\n","        0.00598  , -0.01663  ,  0.10504  , -0.0408   ,  0.0409   ,\n","       -0.03204  , -0.00766  , -0.0664   , -0.02983  , -0.008224 ],\n","      dtype=float16)>\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/mcmc/internal/util.py:363: UserWarning: `step_size` is not a `tf.Tensor`, Python number, or Numpy array. If this parameter is mutable (e.g., a `tf.Variable`), then the behavior implied by `store_parameters_in_results` will silently change on 2019-08-01. Please consult the docstring for `store_parameters_in_results` details and use `store_parameters_in_results=True` to silence this warning.\n","  warnings.warn(\n"]}],"source":["# Set up HMC\n","num_results = 1000\n","num_burnin_steps = 1000\n","\n","# Initialize parameters\n","x = len(list(node_attrs.values())[0])\n","initial_state = [0]*x*2 + [0, 0]  #x_i, x_j , si, sj\n","initial_state = tf.Variable(parameter_initializer([len(initial_state)], dtype=tf.float16))\n","# initial_state = tf.Variable(initial_state)\n","print(f'{initial_state=}')\n","\n","# Define the HMC transition kernel\n","step_size = tf.Variable(0.01)\n","adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n","    tfp.mcmc.HamiltonianMonteCarlo(\n","        target_log_prob_fn=log_likelihood_optimized,\n","        num_leapfrog_steps=10,\n","        step_size=step_size),\n","    num_adaptation_steps=int(num_burnin_steps * 0.8))"],"metadata":{"ExecuteTime":{"end_time":"2024-10-23T04:15:22.424079Z","start_time":"2024-10-23T04:15:15.903675Z"},"id":"27894d3a3e67e98b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732161760371,"user_tz":300,"elapsed":117,"user":{"displayName":"Stein Oyewole","userId":"13069745930786112792"}},"outputId":"83afe8a3-8b51-4218-d47e-0be9e4e9ea64"},"id":"27894d3a3e67e98b","execution_count":null},{"cell_type":"code","outputs":[{"output_type":"stream","name":"stdout","text":["VectorizedH(k_dim=1)\n","\n","Lookup Values:\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.18]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.919]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.984]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.533]\n","  x=0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.814]\n","  x=0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.681]\n","  x=0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.912]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[2.664]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[6.52]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[6.188]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[5.44]\n","  x=0.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[7.113]\n","  x=0.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[5.92]\n","  x=0.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[6.594]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[2.4]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000: H=[0.6665]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[3.]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[2.]\n","  x=1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[6.25]\n","  x=1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[8.836]\n","  x=1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[3.076]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[11.664]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[10.]\n","  x=1.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[23.]\n","  x=1.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[16.17]\n","  x=1.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[12.63]\n","\n","Summary:\n","  Total unique x values: 26\n","VectorizedH(k_dim=1)\n","Unique x values: 26\n","Average H values: 5.852\n","Computed V=<tf.Tensor: shape=(343, 8353), dtype=float16, numpy=\n","array([[-0.628  , -0.4565 , -0.3306 , ..., -0.628  , -0.581  , -0.618  ],\n","       [-0.2167 , -0.0456 ,  0.08044, ..., -0.2167 , -0.17   , -0.2073 ],\n","       [-0.2241 , -0.0531 ,  0.073  , ..., -0.2241 , -0.1775 , -0.2148 ],\n","       ...,\n","       [-0.6963 , -0.5254 , -0.399  , ..., -0.6963 , -0.6494 , -0.6865 ],\n","       [-0.3271 , -0.156  , -0.02998, ..., -0.3271 , -0.2803 , -0.3179 ],\n","       [-0.2852 , -0.11414,  0.01193, ..., -0.2852 , -0.2385 , -0.276  ]],\n","      dtype=float16)> \n"," Using theta=<tf.Variable 'Variable:0' shape=(20,) dtype=float16, numpy=\n","array([-0.0291   ,  0.04324  , -0.0783   ,  0.00989  ,  0.03323  ,\n","       -0.04358  ,  0.01701  ,  0.00419  , -0.06586  , -0.0003023,\n","        0.00598  , -0.01663  ,  0.10504  , -0.0408   ,  0.0409   ,\n","       -0.03204  , -0.00766  , -0.0664   , -0.02983  , -0.008224 ],\n","      dtype=float16)>\n"]},{"output_type":"stream","name":"stderr","text":["H_star: 100%|██████████| 20/20 [01:21<00:00,  4.06s/it]\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=-2858.459>"]},"metadata":{},"execution_count":10}],"source":["weights = {node_id: calculate_weights(s > 0, node_stats.values()) for node_id, s in node_stats.items() }\n","\n","ll, H = log_likelihood_optimized(initial_state, node_attrs, node_stats, weights)\n","ll"],"metadata":{"id":"aeb74e9ae7f8c367","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732161967804,"user_tz":300,"elapsed":207434,"user":{"displayName":"Stein Oyewole","userId":"13069745930786112792"}},"outputId":"5021f92f-a646-42de-b8aa-1a87666bf787"},"id":"aeb74e9ae7f8c367","execution_count":null},{"cell_type":"code","source":["print(repr(H))\n","del ll, H"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUDFN5ckFGyu","executionInfo":{"status":"ok","timestamp":1732161967806,"user_tz":300,"elapsed":6,"user":{"displayName":"Stein Oyewole","userId":"13069745930786112792"}},"outputId":"2cf72a27-4e57-4489-cdba-525375970e2e"},"id":"vUDFN5ckFGyu","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["VectorizedH(k_dim=1)\n","\n","Lookup Values:\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.116]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.095]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.1045]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.204]\n","  x=0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.074]\n","  x=0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.252]\n","  x=0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.876]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.044]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[0.9844]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.109]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.157]\n","  x=0.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.011]\n","  x=0.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.145]\n","  x=0.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.756]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.315]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000: H=[1.239]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.342]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.34]\n","  x=1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.168]\n","  x=1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.162]\n","  x=1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.015]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.049]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.086]\n","  x=1.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[0.6377]\n","  x=1.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.046]\n","  x=1.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.823]\n","\n","Summary:\n","  Total unique x values: 26\n","VectorizedH(k_dim=1)\n","Unique x values: 26\n","Average H values: 1.083\n"]}]},{"cell_type":"code","source":["class MyHMCMC:\n","    def __init__(self, num_dims_theta, num_dims_h=1, num_chains=2, verbosity=True, lr=1e-4, clip_val=5):\n","        self.num_dims_h = num_dims_h\n","        self.num_chains = num_chains\n","        self.verbosity = verbosity\n","        self.num_dims_theta = num_dims_theta\n","        self.param_state = None\n","        self.current_H = None\n","        self.current_ll = None\n","        self.learning_rate = lr\n","        self.clip_val = clip_val\n","\n","    def log_likelihood_wrapper(self, node_attrs, node_stats, weights):\n","        @tf.function\n","        def log_prob(theta):\n","\n","            with tf.GradientTape() as tape:\n","                tape.watch(theta)\n","                # theta = tf.clip_by_value(theta, -5.0, 5.0)\n","                ll, H = log_likelihood_optimized(theta, node_attrs, node_stats, weights, H=self.current_H)\n","\n","            # Adding gradient clipping\n","            grads = tape.gradient(ll, theta)\n","            clipped_grads = tf.clip_by_value([grads], -self.clip_val, self.clip_val)[0]\n","\n","            # Update H and likelihood\n","            self.current_H = H\n","            self.current_ll = ll\n","            if self.verbosity:\n","                print(f\"Log Likelihood: {ll}\")\n","                print(f\"H: {repr(H)}\")\n","                # print(f\"Theta: {theta}\")\n","            return ll / 1000 # lieklihood is still too high normalize to allow model to explore its parameter space\n","        return log_prob\n","\n","    def run_chain(self, node_attrs, node_stats, weights, burn_in_steps=100, num_results = 20):\n","        assert self.param_state is not None, 'INitialize parameters first'\n","\n","        tf.keras.backend.clear_session()\n","\n","        # Define HMCMC kernel\n","        step_size = tf.fill([self.num_dims_theta], self.learning_rate) #[0.1, 0.1, ...]\n","        adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n","            tfp.mcmc.HamiltonianMonteCarlo(\n","                target_log_prob_fn=self.log_likelihood_wrapper(\n","                    node_attrs, node_stats, weights\n","                ),\n","                num_leapfrog_steps=5,\n","                step_size=step_size),\n","            num_adaptation_steps=int(burn_in_steps * 0.8))\n","\n","        # Run the chain\n","        # samples, [final_kernel_results] = tfp.mcmc.sample_chain(\n","        samples = tfp.mcmc.sample_chain(\n","            num_results=num_results,\n","            num_burnin_steps=burn_in_steps,\n","            current_state=self.param_state,\n","            kernel=adaptive_hmc,\n","            trace_fn=None,#,lambda _, pkr: [pkr],\n","            return_final_kernel_results=False)\n","\n","        return samples#, final_kernel_results\n","\n","    def optimize(self, node_attrs, node_stats, weights, burn_in_steps=100, num_results = 20):\n","        # Initialize parameters\n","        # self.param_state = tf.abs(parameter_initializer([self.num_dims_theta], dtype=tf.float32))\n","\n","        # Run the HMC Chain\n","        # samples, final_kernel_results = self.run_chain(node_attrs, node_stats, weights, burn_in_steps, num_results)\n","        samples= self.run_chain(node_attrs, node_stats, weights, burn_in_steps, num_results)\n","        # log probability for sampled params\n","        log_probs = []\n","        for theta_sample in samples:\n","            ll, _ = log_likelihood_optimized(\n","                theta_sample, node_attrs, node_stats, weights, H=self.current_H\n","            )\n","            log_probs.append(ll)\n","\n","        if self.verbosity:\n","            print(f\"Log Likelihood: {log_probs}\")\n","            print(f\"H: {repr(self.current_H)}\")\n","            # print(f\"Theta: {samples}\")\n","        # # Find the best sample\n","        best_idx = tf.argmax(log_probs)\n","        best_sample = samples[best_idx]\n","\n","        return best_sample, samples, log_probs\n","        # return samples, log_probs\n","\n","    def optimize_w_mle(self, node_attrs, node_stats, weights, num_epochs):\n","        self.param_state = tf.Variable(parameter_initializer([self.num_dims_theta], dtype=tf.float32))\n","        optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n","        ll_function = self.log_likelihood_wrapper(node_attrs, node_stats, weights)\n","        losses = []\n","\n","        def train_step():\n","            with tf.GradientTape() as tape:\n","                # optimizer minimizes a loss function but we want to maximize the log likelihood\n","                ll = ll_function(self.param_state)\n","\n","            # Get gradients and update parameters\n","            grads = tape.gradient(ll, [self.param_state])\n","            clipped_grads = [tf.clip_by_value(g, -self.clip_val, self.clip_val) for g in grads]\n","            print(f\"{clipped_grads=}, {grads=}\")\n","            optimizer.apply_gradients(zip(clipped_grads, [self.param_state]))\n","\n","            return ll\n","\n","\n","        for epoch in range(num_epochs):\n","            ll = train_step()\n","            gc.collect()\n","            losses.append(ll)\n","            if self.verbosity:\n","              print(f\"Epoch {epoch+1}, Log Likelihood: {-1*ll}\")\n","\n","        return losses"],"metadata":{"id":"POtfXX_hgfln"},"id":"POtfXX_hgfln","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up HMC\n","num_results = 20\n","num_burnin_steps = 100\n","\n","# Initialize parameters\n","x = len(list(node_attrs.values())[0])\n","dim_theta = x*2 + 2\n","\n","\n","hmcmc_optimizer = MyHMCMC(num_dims_theta=dim_theta, )"],"metadata":{"id":"Xe8I8__bCraA"},"id":"Xe8I8__bCraA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["theta = hmcmc_optimizer.optimize_w_mle(node_attrs, node_stats, weights, num_epochs=20)\n","theta, hmcmc_optimizer.param_state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JN7KDGxzNhMX","outputId":"6ca37988-847e-476d-ac61-3104e884ec58"},"id":"JN7KDGxzNhMX","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["VectorizedH(k_dim=1)\n","\n","Lookup Values:\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.18]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.919]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.984]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.533]\n","  x=0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.814]\n","  x=0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.681]\n","  x=0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.912]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[2.664]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[6.52]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[6.188]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[5.44]\n","  x=0.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[7.113]\n","  x=0.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[5.92]\n","  x=0.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[6.594]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[2.4]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000: H=[0.6665]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[3.]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[2.]\n","  x=1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[6.25]\n","  x=1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[8.836]\n","  x=1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[3.076]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[11.664]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[10.]\n","  x=1.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[23.]\n","  x=1.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[16.17]\n","  x=1.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[12.63]\n","\n","Summary:\n","  Total unique x values: 26\n","VectorizedH(k_dim=1)\n","Unique x values: 26\n","Average H values: 5.852\n","Computed V=<tf.Tensor: shape=(343, 8353), dtype=float16, numpy=\n","array([[-0.628  , -0.4565 , -0.3306 , ..., -0.628  , -0.581  , -0.618  ],\n","       [-0.2167 , -0.04565,  0.08044, ..., -0.2167 , -0.17   , -0.2073 ],\n","       [-0.2241 , -0.0531 ,  0.073  , ..., -0.2241 , -0.1775 , -0.2148 ],\n","       ...,\n","       [-0.6963 , -0.525  , -0.399  , ..., -0.6963 , -0.6494 , -0.6865 ],\n","       [-0.3271 , -0.156  , -0.02992, ..., -0.3271 , -0.2803 , -0.3176 ],\n","       [-0.2852 , -0.11414,  0.01197, ..., -0.2852 , -0.2385 , -0.276  ]],\n","      dtype=float16)> \n"," Using theta=<tf.Tensor: shape=(20,), dtype=float16, numpy=\n","array([-0.0291   ,  0.04327  , -0.0783   ,  0.009895 ,  0.03323  ,\n","       -0.04358  ,  0.01701  ,  0.004192 , -0.06586  , -0.0003023,\n","        0.005985 , -0.01663  ,  0.10504  , -0.04083  ,  0.04092  ,\n","       -0.03204  , -0.00766  , -0.0664   , -0.02983  , -0.008224 ],\n","      dtype=float16)>\n"]},{"output_type":"stream","name":"stderr","text":["H_star: 100%|██████████| 20/20 [01:18<00:00,  3.95s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Log Likelihood: -2858.306640625\n","H: VectorizedH(k_dim=1)\n","\n","Lookup Values:\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.116]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.095]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.1045]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.204]\n","  x=0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.074]\n","  x=0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.253]\n","  x=0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.876]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.044]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[0.9844]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.109]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.157]\n","  x=0.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.011]\n","  x=0.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.145]\n","  x=0.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.756]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.315]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000: H=[1.239]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.342]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.34]\n","  x=1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.168]\n","  x=1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.162]\n","  x=1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.015]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.049]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.086]\n","  x=1.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[0.6377]\n","  x=1.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.046]\n","  x=1.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.823]\n","\n","Summary:\n","  Total unique x values: 26\n","VectorizedH(k_dim=1)\n","Unique x values: 26\n","Average H values: 1.083\n","clipped_grads=[<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([3.5546875e-01, 4.1835938e+00, 4.4843750e+00, 9.4970703e-01,\n","       5.8398438e-01, 1.9458008e-01, 2.7587891e-01, 1.0664062e+00,\n","       9.7656250e-04, 3.6914062e-01, 4.3476562e+00, 4.4765625e+00,\n","       9.4580078e-01, 5.8007812e-01, 1.9433594e-01, 2.7685547e-01,\n","       1.0644531e+00, 1.0528564e-03, 5.0000000e+00, 5.0000000e+00],\n","      dtype=float32)>], grads=[<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([3.5546875e-01, 4.1835938e+00, 4.4843750e+00, 9.4970703e-01,\n","       5.8398438e-01, 1.9458008e-01, 2.7587891e-01, 1.0664062e+00,\n","       9.7656250e-04, 3.6914062e-01, 4.3476562e+00, 4.4765625e+00,\n","       9.4580078e-01, 5.8007812e-01, 1.9433594e-01, 2.7685547e-01,\n","       1.0644531e+00, 1.0528564e-03, 7.3562500e+01, 7.5625000e+01],\n","      dtype=float32)>]\n","Epoch 1, Log Likelihood: 2.858306646347046\n","Computed V=<tf.Tensor: shape=(343, 8353), dtype=float16, numpy=\n","array([[-0.631  , -0.459  , -0.3335 , ..., -0.631  , -0.584  , -0.621  ],\n","       [-0.2177 , -0.04602,  0.0795 , ..., -0.2177 , -0.1708 , -0.2081 ],\n","       [-0.2251 , -0.05353,  0.072  , ..., -0.2251 , -0.1782 , -0.2156 ],\n","       ...,\n","       [-0.7    , -0.5283 , -0.4028 , ..., -0.7    , -0.6533 , -0.6904 ],\n","       [-0.329  , -0.1575 , -0.03192, ..., -0.329  , -0.2822 , -0.3196 ],\n","       [-0.2864 , -0.11475,  0.01079, ..., -0.2864 , -0.2395 , -0.2769 ]],\n","      dtype=float16)> \n"," Using theta=<tf.Tensor: shape=(20,), dtype=float16, numpy=\n","array([-0.02919  ,  0.04318  , -0.0784   ,  0.00979  ,  0.03314  ,\n","       -0.04367  ,  0.0169   ,  0.00409  , -0.066    , -0.0004022,\n","        0.005886 , -0.01674  ,  0.1049   , -0.04092  ,  0.0408   ,\n","       -0.03217  , -0.007763 , -0.0665   , -0.02992  , -0.00832  ],\n","      dtype=float16)>\n"]},{"output_type":"stream","name":"stderr","text":["H_star:  30%|███       | 6/20 [00:26<01:02,  4.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Convergence achieved after 6 iterations.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Log Likelihood: -2854.419921875\n","H: VectorizedH(k_dim=1)\n","\n","Lookup Values:\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.114]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.093]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.103]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.202]\n","  x=0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.072]\n","  x=0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.25]\n","  x=0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.874]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.041]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[0.9814]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.107]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.154]\n","  x=0.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.008]\n","  x=0.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.142]\n","  x=0.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.7534]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.313]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000: H=[1.237]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.34]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.339]\n","  x=1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.165]\n","  x=1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.16]\n","  x=1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.013]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.046]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.083]\n","  x=1.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[0.6343]\n","  x=1.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.043]\n","  x=1.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.8203]\n","\n","Summary:\n","  Total unique x values: 26\n","VectorizedH(k_dim=1)\n","Unique x values: 26\n","Average H values: 1.080\n","clipped_grads=[<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([3.5546875e-01, 4.1835938e+00, 4.4843750e+00, 9.4970703e-01,\n","       5.8398438e-01, 1.9458008e-01, 2.7587891e-01, 1.0664062e+00,\n","       9.7656250e-04, 3.6914062e-01, 4.3476562e+00, 4.4765625e+00,\n","       9.4580078e-01, 5.8007812e-01, 1.9433594e-01, 2.7685547e-01,\n","       1.0644531e+00, 1.0528564e-03, 5.0000000e+00, 5.0000000e+00],\n","      dtype=float32)>], grads=[<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([3.5546875e-01, 4.1835938e+00, 4.4843750e+00, 9.4970703e-01,\n","       5.8398438e-01, 1.9458008e-01, 2.7587891e-01, 1.0664062e+00,\n","       9.7656250e-04, 3.6914062e-01, 4.3476562e+00, 4.4765625e+00,\n","       9.4580078e-01, 5.8007812e-01, 1.9433594e-01, 2.7685547e-01,\n","       1.0644531e+00, 1.0528564e-03, 7.3562500e+01, 7.5625000e+01],\n","      dtype=float32)>]\n","Epoch 2, Log Likelihood: 2.8544199466705322\n","Computed V=<tf.Tensor: shape=(343, 8353), dtype=float16, numpy=\n","array([[-0.634  , -0.4614 , -0.3364 , ..., -0.634  , -0.5864 , -0.624  ],\n","       [-0.2186 , -0.04642,  0.0785 , ..., -0.2186 , -0.1715 , -0.209  ],\n","       [-0.2261 , -0.0539 ,  0.07104, ..., -0.2261 , -0.179  , -0.2163 ],\n","       ...,\n","       [-0.704  , -0.5317 , -0.407  , ..., -0.704  , -0.6567 , -0.6943 ],\n","       [-0.331  , -0.1589 , -0.03394, ..., -0.331  , -0.284  , -0.3213 ],\n","       [-0.2876 , -0.11536,  0.00958, ..., -0.2876 , -0.2405 , -0.2778 ]],\n","      dtype=float16)> \n"," Using theta=<tf.Tensor: shape=(20,), dtype=float16, numpy=\n","array([-0.0293  ,  0.04306 , -0.0785  ,  0.00969 ,  0.03305 , -0.04376 ,\n","        0.01682 ,  0.00399 , -0.06604 , -0.000502,  0.005787, -0.01685 ,\n","        0.10486 , -0.04102 ,  0.0407  , -0.03226 , -0.00786 , -0.06665 ,\n","       -0.03003 , -0.00842 ], dtype=float16)>\n"]},{"output_type":"stream","name":"stderr","text":["H_star:  30%|███       | 6/20 [00:27<01:03,  4.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Convergence achieved after 6 iterations.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Log Likelihood: -2851.677734375\n","H: VectorizedH(k_dim=1)\n","\n","Lookup Values:\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.112]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.091]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.101]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.2]\n","  x=0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.069]\n","  x=0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.247]\n","  x=0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.8716]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.038]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[0.9785]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.1045]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.152]\n","  x=0.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.005]\n","  x=0.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.138]\n","  x=0.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.7505]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.3125]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000: H=[1.236]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.339]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.337]\n","  x=1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.163]\n","  x=1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.157]\n","  x=1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.011]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.043]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.08]\n","  x=1.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[0.631]\n","  x=1.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.04]\n","  x=1.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.8174]\n","\n","Summary:\n","  Total unique x values: 26\n","VectorizedH(k_dim=1)\n","Unique x values: 26\n","Average H values: 1.078\n","clipped_grads=[<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([3.5546875e-01, 4.1835938e+00, 4.4843750e+00, 9.4970703e-01,\n","       5.8398438e-01, 1.9458008e-01, 2.7587891e-01, 1.0664062e+00,\n","       9.7656250e-04, 3.6914062e-01, 4.3476562e+00, 4.4765625e+00,\n","       9.4580078e-01, 5.8007812e-01, 1.9433594e-01, 2.7685547e-01,\n","       1.0644531e+00, 1.0528564e-03, 5.0000000e+00, 5.0000000e+00],\n","      dtype=float32)>], grads=[<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([3.5546875e-01, 4.1835938e+00, 4.4843750e+00, 9.4970703e-01,\n","       5.8398438e-01, 1.9458008e-01, 2.7587891e-01, 1.0664062e+00,\n","       9.7656250e-04, 3.6914062e-01, 4.3476562e+00, 4.4765625e+00,\n","       9.4580078e-01, 5.8007812e-01, 1.9433594e-01, 2.7685547e-01,\n","       1.0644531e+00, 1.0528564e-03, 7.3562500e+01, 7.5625000e+01],\n","      dtype=float32)>]\n","Epoch 3, Log Likelihood: 2.851677656173706\n","Computed V=<tf.Tensor: shape=(343, 8353), dtype=float16, numpy=\n","array([[-0.6367 , -0.4636 , -0.3394 , ..., -0.6367 , -0.589  , -0.6265 ],\n","       [-0.2196 , -0.04678,  0.0775 , ..., -0.2196 , -0.1722 , -0.2096 ],\n","       [-0.227  , -0.05426,  0.07007, ..., -0.227  , -0.1799 , -0.2173 ],\n","       ...,\n","       [-0.708  , -0.535  , -0.4106 , ..., -0.708  , -0.6606 , -0.6978 ],\n","       [-0.333  , -0.1602 , -0.0359 , ..., -0.333  , -0.2856 , -0.323  ],\n","       [-0.2888 , -0.1159 ,  0.00841, ..., -0.2888 , -0.2415 , -0.2788 ]],\n","      dtype=float16)> \n"," Using theta=<tf.Tensor: shape=(20,), dtype=float16, numpy=\n","array([-0.02939  ,  0.04297  , -0.0786   ,  0.00959  ,  0.03296  ,\n","       -0.04385  ,  0.01671  ,  0.003891 , -0.06616  , -0.0006022,\n","        0.005684 , -0.01694  ,  0.10474  , -0.0411   ,  0.04062  ,\n","       -0.03235  , -0.007965 , -0.0667   , -0.03012  , -0.00852  ],\n","      dtype=float16)>\n"]},{"output_type":"stream","name":"stderr","text":["H_star:  30%|███       | 6/20 [00:26<01:02,  4.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Convergence achieved after 6 iterations.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Log Likelihood: -2844.71044921875\n","H: VectorizedH(k_dim=1)\n","\n","Lookup Values:\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.11]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.088]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.099]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.198]\n","  x=0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.067]\n","  x=0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.245]\n","  x=0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.869]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.036]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[0.9756]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.102]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.149]\n","  x=0.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.002]\n","  x=0.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.135]\n","  x=0.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.748]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.312]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000: H=[1.235]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.337]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.335]\n","  x=1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.161]\n","  x=1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.154]\n","  x=1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.009]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.041]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.078]\n","  x=1.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[0.6274]\n","  x=1.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.036]\n","  x=1.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.815]\n","\n","Summary:\n","  Total unique x values: 26\n","VectorizedH(k_dim=1)\n","Unique x values: 26\n","Average H values: 1.075\n","clipped_grads=[<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([3.5546875e-01, 4.1835938e+00, 4.4843750e+00, 9.4970703e-01,\n","       5.8398438e-01, 1.9458008e-01, 2.7587891e-01, 1.0664062e+00,\n","       9.7656250e-04, 3.6914062e-01, 4.3476562e+00, 4.4765625e+00,\n","       9.4580078e-01, 5.8007812e-01, 1.9433594e-01, 2.7685547e-01,\n","       1.0644531e+00, 1.0528564e-03, 5.0000000e+00, 5.0000000e+00],\n","      dtype=float32)>], grads=[<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([3.5546875e-01, 4.1835938e+00, 4.4843750e+00, 9.4970703e-01,\n","       5.8398438e-01, 1.9458008e-01, 2.7587891e-01, 1.0664062e+00,\n","       9.7656250e-04, 3.6914062e-01, 4.3476562e+00, 4.4765625e+00,\n","       9.4580078e-01, 5.8007812e-01, 1.9433594e-01, 2.7685547e-01,\n","       1.0644531e+00, 1.0528564e-03, 7.3562500e+01, 7.5625000e+01],\n","      dtype=float32)>]\n","Epoch 4, Log Likelihood: 2.844710350036621\n","Computed V=<tf.Tensor: shape=(343, 8353), dtype=float16, numpy=\n","array([[-0.6396  , -0.4663  , -0.3423  , ..., -0.6396  , -0.592   ,\n","        -0.6294  ],\n","       [-0.2207  , -0.04724 ,  0.07654 , ..., -0.2207  , -0.1732  ,\n","        -0.2106  ],\n","       [-0.2283  , -0.0547  ,  0.0691  , ..., -0.2283  , -0.1807  ,\n","        -0.218   ],\n","       ...,\n","       [-0.712   , -0.5386  , -0.4148  , ..., -0.712   , -0.6646  ,\n","        -0.702   ],\n","       [-0.335   , -0.1616  , -0.03784 , ..., -0.335   , -0.2876  ,\n","        -0.325   ],\n","       [-0.29    , -0.1166  ,  0.007233, ..., -0.29    , -0.2424  ,\n","        -0.2798  ]], dtype=float16)> \n"," Using theta=<tf.Tensor: shape=(20,), dtype=float16, numpy=\n","array([-0.0295   ,  0.04288  , -0.07874  ,  0.00949  ,  0.03284  ,\n","       -0.04398  ,  0.01662  ,  0.003792 , -0.0663   , -0.0007024,\n","        0.005585 , -0.01704  ,  0.1047   , -0.04123  ,  0.04053  ,\n","       -0.03244  , -0.008064 , -0.06683  , -0.03023  , -0.00862  ],\n","      dtype=float16)>\n"]},{"output_type":"stream","name":"stderr","text":["H_star:  50%|█████     | 10/20 [00:42<00:42,  4.21s/it]"]},{"output_type":"stream","name":"stdout","text":["Convergence achieved after 10 iterations.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Log Likelihood: -2837.7724609375\n","H: VectorizedH(k_dim=1)\n","\n","Lookup Values:\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.108]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.086]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.096]\n","  x=0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.195]\n","  x=0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.064]\n","  x=0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.242]\n","  x=0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.8667]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.033]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[0.9727]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.099]\n","  x=0.000000_1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000: H=[1.146]\n","  x=0.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[0.999]\n","  x=0.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.132]\n","  x=0.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.745]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.31]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000: H=[1.234]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.335]\n","  x=1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000: H=[1.334]\n","  x=1.000000_0.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[1.159]\n","  x=1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.151]\n","  x=1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.006]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.038]\n","  x=1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_1.000000_0.000000: H=[1.075]\n","  x=1.000000_1.000000_0.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000: H=[0.6235]\n","  x=1.000000_1.000000_0.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[1.033]\n","  x=1.000000_1.000000_1.000000_0.000000_0.000000_0.000000_0.000000_0.000000_0.000000: H=[0.812]\n","\n","Summary:\n","  Total unique x values: 26\n","VectorizedH(k_dim=1)\n","Unique x values: 26\n","Average H values: 1.073\n","clipped_grads=[<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([3.5546875e-01, 4.1835938e+00, 4.4843750e+00, 9.4970703e-01,\n","       5.8398438e-01, 1.9458008e-01, 2.7587891e-01, 1.0664062e+00,\n","       9.7656250e-04, 3.6914062e-01, 4.3476562e+00, 4.4765625e+00,\n","       9.4580078e-01, 5.8007812e-01, 1.9433594e-01, 2.7685547e-01,\n","       1.0644531e+00, 1.0528564e-03, 5.0000000e+00, 5.0000000e+00],\n","      dtype=float32)>], grads=[<tf.Tensor: shape=(20,), dtype=float32, numpy=\n","array([3.5546875e-01, 4.1835938e+00, 4.4843750e+00, 9.4970703e-01,\n","       5.8398438e-01, 1.9458008e-01, 2.7587891e-01, 1.0664062e+00,\n","       9.7656250e-04, 3.6914062e-01, 4.3476562e+00, 4.4765625e+00,\n","       9.4580078e-01, 5.8007812e-01, 1.9433594e-01, 2.7685547e-01,\n","       1.0644531e+00, 1.0528564e-03, 7.3562500e+01, 7.5625000e+01],\n","      dtype=float32)>]\n","Epoch 5, Log Likelihood: 2.8377723693847656\n","Computed V=<tf.Tensor: shape=(343, 8353), dtype=float16, numpy=\n","array([[-0.6426  , -0.4688  , -0.3457  , ..., -0.6426  , -0.5947  ,\n","        -0.6323  ],\n","       [-0.2217  , -0.04764 ,  0.07544 , ..., -0.2217  , -0.174   ,\n","        -0.2113  ],\n","       [-0.2292  , -0.0551  ,  0.068   , ..., -0.2292  , -0.1814  ,\n","        -0.2188  ],\n","       ...,\n","       [-0.716   , -0.542   , -0.4192  , ..., -0.716   , -0.6685  ,\n","        -0.706   ],\n","       [-0.3372  , -0.1631  , -0.03998 , ..., -0.3372  , -0.2896  ,\n","        -0.3267  ],\n","       [-0.2913  , -0.1172  ,  0.005936, ..., -0.2913  , -0.2434  ,\n","        -0.2808  ]], dtype=float16)> \n"," Using theta=<tf.Tensor: shape=(20,), dtype=float16, numpy=\n","array([-0.02959 ,  0.04276 , -0.0788  ,  0.00939 ,  0.03275 , -0.04407 ,\n","        0.01651 ,  0.00369 , -0.06635 , -0.000802,  0.005486, -0.01714 ,\n","        0.10455 , -0.04132 ,  0.0404  , -0.03256 , -0.00816 , -0.0669  ,\n","       -0.03033 , -0.00873 ], dtype=float16)>\n"]},{"output_type":"stream","name":"stderr","text":["H_star: 100%|██████████| 20/20 [01:18<00:00,  3.91s/it]\n"]}]},{"cell_type":"code","source":["all_params, log_likelihood = hmcmc_optimizer.optimize(\n","    node_attrs, node_stats, weights,\n","    num_results=num_results, burn_in_steps=num_burnin_steps\n",")"],"metadata":{"id":"Wu2uYsHVKTSe"},"id":"Wu2uYsHVKTSe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["weights"],"metadata":{"id":"9cv0B9_CKkVu"},"id":"9cv0B9_CKkVu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","gc.collect()"],"metadata":{"id":"Nn3x9jlhXkcx"},"id":"Nn3x9jlhXkcx","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"leZyNNf4b1Vb"},"id":"leZyNNf4b1Vb","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}