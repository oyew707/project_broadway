{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNO994iLiyd44DdT2fuS9nG"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Network Formation with Hamiltonian Markov Chain Monte Carlo\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This project analyzes the formation and structure of trust networks in historical financial markets, using data from the New York Stock Exchange (NYSE) spanning 1883 to 1930. We explore how social connections, ethnicity, and latent characteristics influenced the sponsorship of new NYSE members and the subsequent approval process by existing members."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s-ZZQPgGHQNU",
    "ExecuteTime": {
     "end_time": "2024-10-16T18:25:47.246692Z",
     "start_time": "2024-10-16T18:25:26.635294Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T17:41:24.632495Z",
     "start_time": "2024-10-16T17:41:24.629050Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the provided data structure and the model described in the document, we can formulate an expressive representation of the limiting likelihood function. Let's define the key components:\n",
    "\n",
    "1. Node attributes:\n",
    "$x_i$ = ($ethnicity_i$, $everCommittee_i$, $everSponsor_i$)\n",
    "2. Edge formation: Let $L_{ij,t}$ be an indicator for whether node i chose node j as a sponsor in transaction t.\n",
    "3. Payoff structure:\n",
    "$U_{ij,t}$ = $U^*(x_i, x_j; s_{i,t}, s_{jt}, t_{ij,t}) + σε_{ij,t}$\n",
    "    Where:\n",
    "    - $s_{it}$, $s_{jt}$ are node-specific network statistics (e.g., degree centrality)\n",
    "    - $t_{ij,t}$ are edge-specific network statistics (e.g., common ethnicity)\n",
    "\n",
    "4. Aggregate state variables:\n",
    "    - $M^*(x, s|R)$: Reference distribution\n",
    "    - $H^*(x; s)$: Inclusive value function\n",
    "    \n",
    "5. Limiting link frequency probability:\n",
    "$μ_0(L_{ij,t} = 1, s_{it}, s_{jt} | x_i, x_j) = \\frac{s_{it} s_{ij,t} *exp(U^*(x_i,x_j;s_{it}, s_{jt}) + U^*(x_j,x_i;s_{jt},s_{it}))}{(1 + H^*(x_i,s_{it}))*(1 + H^*(x_j,s_{jt}))} \\times M^*(x_i,s_{it}|R) * M^*(x_j,s_{jt}|R)$\n",
    "\n",
    "6. Committee voting: Let $v_{kt}$ be the vote (white ball or black ball) of committee member k in transaction t. $$P(v_{kt} = white | x_i, x_{j1}, x_{j2}, x_k) = Φ(αx_i + β_1x_{j1} + β_2x_{j2} + γx_k + δ_1d(Z_i, Z_k) + δ_2d(Z_{j1}, Z_k) + δ_3d(Z_{j2}, Z_k))$$\n",
    "Where:\n",
    "    - Φ is the standard normal CDF\n",
    "    - $Z_i$, $Z_{j1}$, $Z_{j2}$, $Z_k$ are latent positions in a low-dimensional Euclidean space\n",
    "    d(·,·) is a distance function in this space.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read the data files\n",
    "node_data = pd.read_csv('data/nyse_node_sp1.csv', header=None, \n",
    "                        names=['name', 'ever_committee', 'node_id', 'ethnicity', 'ever_sponsor'])\n",
    "edge_data = pd.read_csv('data/nyse_edge_buy_sp_sp1.csv', header=None,\n",
    "                        names=['buyer_id', 'sponsor1_id', 'sponsor2_id', 'f1', 'f2', 'f3', 'f4', 'blackballs', 'whiteballs', 'year'])\n",
    "committee_data = pd.read_csv('data/nyse_edge_buy_com1.csv', header=None,\n",
    "                             names=['buyer_id', 'committee_id', 'f1', 'f2', 'f3', 'f4', 'blackballs', 'whiteballs', 'year'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T18:26:14.233196Z",
     "start_time": "2024-10-16T18:26:14.183824Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def process_data(node_data, edge_data, committee_data):\n",
    "    node_attrs = node_data.set_index('node_id').to_dict('index')\n",
    "    \n",
    "    # Initialize network statistics\n",
    "    network_stats = {node_id: {'degree': 0, 'sponsor_count': 0} for node_id in node_attrs}\n",
    "    \n",
    "    transactions = []\n",
    "    for _, row in edge_data.iterrows():\n",
    "        buyer_id = row['buyer_id']\n",
    "        sponsor1_id = row['sponsor1_id']\n",
    "        sponsor2_id = row['sponsor2_id']\n",
    "        year = row['year']\n",
    "        \n",
    "        # Update network statistics\n",
    "        network_stats[buyer_id]['degree'] += 2\n",
    "        network_stats[sponsor1_id]['degree'] += 1\n",
    "        network_stats[sponsor2_id]['degree'] += 1\n",
    "        network_stats[sponsor1_id]['sponsor_count'] += 1\n",
    "        network_stats[sponsor2_id]['sponsor_count'] += 1\n",
    "        \n",
    "        committee_members = committee_data[(committee_data['buyer_id'] == buyer_id) & \n",
    "                                           (committee_data['year'] == year)]['committee_id'].tolist()\n",
    "        \n",
    "        transactions.append({\n",
    "            'buyer_id': buyer_id,\n",
    "            'sponsor1_id': sponsor1_id,\n",
    "            'sponsor2_id': sponsor2_id,\n",
    "            'committee_members': committee_members,\n",
    "            'year': year,\n",
    "            'whiteballs': row['whiteballs'],\n",
    "            'blackballs': row['blackballs']\n",
    "        })\n",
    "    \n",
    "    return node_attrs, transactions, network_stats\n",
    "\n",
    "node_attrs, transactions, network_stats = process_data(node_data, edge_data, committee_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T18:26:20.445284Z",
     "start_time": "2024-10-16T18:26:19.435130Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define utility function\n",
    "@tf.function\n",
    "def U_star(xi, xj, si, sj, tij, theta):\n",
    "    return tf.tensordot(theta, tf.concat([xi, xj, si, sj, tij], axis=0), axes=1)\n",
    "\n",
    "# Define reference distribution\n",
    "@tf.function\n",
    "def M_star(x, s, R, eta):\n",
    "    return tf.exp(tf.tensordot(eta, tf.concat([x, s, R], axis=0), axes=1))\n",
    "\n",
    "# Inlcusive value function\n",
    "@tf.function\n",
    "def H_star(x, s, gamma):\n",
    "    return tf.exp(tf.tensordot(gamma, tf.concat([x, s], axis=0), axes=1))\n",
    "\n",
    "# Define limiting link probability\n",
    "@tf.function\n",
    "def mu_star_0(Lijt, sit, sjt, xi, xj, R, theta, eta, gamma):\n",
    "    numerator = sit * sjt * tf.exp(U_star(xi, xj, sit, sjt, tf.constant([]), theta) + \n",
    "                                   U_star(xj, xi, sjt, sit, tf.constant([]), theta))\n",
    "    denominator = (1 + H_star(xi, sit, gamma)) * (1 + H_star(xj, sjt, gamma))\n",
    "    prob = (numerator / denominator) * M_star(xi, sit, R, eta) * M_star(xj, sjt, R, eta)\n",
    "    \n",
    "    # Use Lijt to determine whether we want P(link = 1) or P(link = 0)\n",
    "    return tf.where(Lijt == 1, prob, 1 - prob) \n",
    "\n",
    "# Define committee voting probability\n",
    "@tf.function\n",
    "def P_vote(v, xi, xj1, xj2, xk, Z, alpha, beta1, beta2, gamma, delta):\n",
    "    latent_term = delta[0]*tf.norm(Z[xi] - Z[xk]) + \\\n",
    "                  delta[1]*tf.norm(Z[xj1] - Z[xk]) + \\\n",
    "                  delta[2]*tf.norm(Z[xj2] - Z[xk])\n",
    "    logits = tf.tensordot(alpha, xi, axes=1) + tf.tensordot(beta1, xj1, axes=1) + \\\n",
    "             tf.tensordot(beta2, xj2, axes=1) + tf.tensordot(gamma, xk, axes=1) - latent_term\n",
    "    prob = tfp.distributions.Normal(loc=0., scale=1.).cdf(logits)\n",
    "    \n",
    "    # Use v to determine whether we want P(vote = 1) or P(vote = 0)\n",
    "    return tf.where(v == 1, prob, 1 - prob)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T17:41:24.632621Z",
     "start_time": "2024-10-16T17:41:24.629481Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Log limiting likelihood function\n",
    "@tf.function\n",
    "def log_likelihood(params, node_attrs, transactions, Z):\n",
    "    theta, eta, gamma, alpha, beta1, beta2, gamma_vote, delta, mu_Z, Sigma_Z = params\n",
    "    \n",
    "    ll = tf.constant(0., dtype=tf.float32)\n",
    "    \n",
    "    for t in transactions:\n",
    "        buyer_id = t['buyer_id']\n",
    "        sponsor1_id = t['sponsor1_id']\n",
    "        sponsor2_id = t['sponsor2_id']\n",
    "        \n",
    "        buyer_stats = tf.constant([network_stats[buyer_id]['degree'], network_stats[buyer_id]['sponsor_count']], dtype=tf.float32)\n",
    "        sponsor1_stats = tf.constant([network_stats[sponsor1_id]['degree'], network_stats[sponsor1_id]['sponsor_count']], dtype=tf.float32)\n",
    "        sponsor2_stats = tf.constant([network_stats[sponsor2_id]['degree'], network_stats[sponsor2_id]['sponsor_count']], dtype=tf.float32)\n",
    "        \n",
    "        ##TODO: what is R?\n",
    "        \n",
    "        # Sponsor choice likelihood\n",
    "        ll += tf.math.log(mu_star_0(tf.constant(1.),  buyer_stats, sponsor1_stats, \n",
    "                                    node_attrs[buyer_id], node_attrs[sponsor1_id], \n",
    "                                    tf.constant([]), theta, eta, gamma))\n",
    "        ll += tf.math.log(mu_star_0(tf.constant(1.),  buyer_stats, sponsor2_stats,\n",
    "                                    node_attrs[buyer_id], node_attrs[sponsor2_id], \n",
    "                                    tf.constant([]), theta, eta, gamma))\n",
    "        \n",
    "        # Add log probability for not choosing other potential sponsors\n",
    "        for potential_sponsor_id in node_attrs:\n",
    "            if potential_sponsor_id not in [sponsor1_id, sponsor2_id]:\n",
    "                ll += tf.math.log(1 - mu_star_0(tf.constant(0.),  buyer_stats, sponsor1_stats, \n",
    "                                                node_attrs[buyer_id], node_attrs[potential_sponsor_id], \n",
    "                                                tf.constant([]), theta, eta, gamma))\n",
    "        \n",
    "        # Committee voting likelihood\n",
    "        for committee_id in t['committee_members']:\n",
    "            v = tf.constant(1. if t['whiteballs'] > t['blackballs'] else 0., dtype=tf.float32)\n",
    "            ll += tf.math.log(P_vote(v, node_attrs[buyer_id], node_attrs[sponsor1_id], \n",
    "                                     node_attrs[sponsor2_id], node_attrs[committee_id], \n",
    "                                     Z, alpha, beta1, beta2, gamma_vote, delta))\n",
    "    \n",
    "    # Add prior for latent positions\n",
    "    for node_id in Z:\n",
    "        ll += tfp.distributions.MultivariateNormalDiag(loc=mu_Z, scale_diag=tf.linalg.diag_part(Sigma_Z)).log_prob(Z[node_id])\n",
    "    \n",
    "    return ll"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set up HMC\n",
    "num_results = 1000\n",
    "num_burnin_steps = 1000\n",
    "\n",
    "# Initialize parameters\n",
    "initial_state = tf.concat([\n",
    "    tf.zeros(10),  # theta\n",
    "    tf.zeros(5),   # eta\n",
    "    tf.zeros(5),   # gamma\n",
    "    tf.zeros(5),   # alpha\n",
    "    tf.zeros(5),   # beta1\n",
    "    tf.zeros(5),   # beta2\n",
    "    tf.zeros(5),   # gamma_vote\n",
    "    tf.zeros(3),   # delta\n",
    "    tf.zeros(2),   # mu_Z\n",
    "    tf.ones(2),    # diag(Sigma_Z)\n",
    "    tf.random.normal([len(node_attrs) * 2])  # Z (flattened)\n",
    "], axis=0)\n",
    "\n",
    "# Define the HMC transition kernel\n",
    "step_size = tf.Variable(0.01)\n",
    "adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "    tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=log_likelihood,\n",
    "        num_leapfrog_steps=10,\n",
    "        step_size=step_size),\n",
    "    num_adaptation_steps=int(num_burnin_steps * 0.8))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
