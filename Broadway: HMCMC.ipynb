{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"O7eA8zObCS0s"},"source":["# Network Formation with Hamiltonian Markov Chain Monte Carlo\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"6eNaOm6sCS0w"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"6RT3G80lCS0w"},"source":["This project analyzes the formation and structure of trust networks in historical financial markets, using data from the New York Stock Exchange (NYSE) spanning 1883 to 1930. We explore how social connections, ethnicity, and latent characteristics influenced the sponsorship of new NYSE members and the subsequent approval process by existing members."]},{"cell_type":"markdown","metadata":{"id":"87-WDHR06xNk"},"source":["## Stein prep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ux_EnaJ26w2Y","ExecuteTime":{"end_time":"2024-10-25T22:52:17.157911Z","start_time":"2024-10-25T22:52:17.132916Z"}},"outputs":[],"source":["try:\n","    from google.colab import drive\n","    import os\n","    drive.mount('/content/drive')\n","    os.chdir('drive/MyDrive/School/DS-GA 1006/code')\n","    print(os.getcwd())\n","except:\n","  pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EmoGqHjP61Vj","ExecuteTime":{"end_time":"2024-10-25T22:52:17.159435Z","start_time":"2024-10-25T22:52:17.144835Z"}},"outputs":[],"source":["# ! pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"eSkvBaJhCS0x"},"source":["### Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-ZZQPgGHQNU","ExecuteTime":{"end_time":"2024-10-25T22:52:21.274478Z","start_time":"2024-10-25T22:52:17.163464Z"}},"outputs":[],"source":["import tensorflow as tf\n","import pandas as pd\n","import tensorflow_probability as tfp\n","from tqdm import tqdm\n","from functools import lru_cache\n","from collections import defaultdict"]},{"cell_type":"markdown","metadata":{"id":"x0ilz8n-66CM"},"source":[]},{"cell_type":"markdown","metadata":{"id":"VqekgFR86qhw"},"source":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"1QWC6FZ2CS0x"},"source":["### Constants"]},{"cell_type":"code","outputs":[],"source":["tf.executing_eagerly()\n","import time\n","from functools import wraps\n","\n","def timeit(func):\n","    @wraps(func)\n","    def wrapper(*args, **kwargs):\n","        start_time = time.perf_counter()\n","        result = func(*args, **kwargs)\n","        end_time = time.perf_counter()\n","        total_time = end_time - start_time\n","        print(f'Function {func.__name__}{args} {kwargs} took {total_time:.4f} seconds')\n","        return result\n","    return wrapper"],"metadata":{"ExecuteTime":{"end_time":"2024-10-25T22:52:21.275747Z","start_time":"2024-10-25T22:52:21.273723Z"},"id":"RkHVSIxb47aM"},"execution_count":null},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"xN5w_295CS0y"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"sNcF4ekACS0y"},"source":["Based on the provided data structure and the model described in the document, we can formulate an expressive representation of the limiting likelihood function. Let's define the key components:\n","\n","1. Node attributes:\n","$x_i$ = ($ethnicity_i$, $everCommittee_i$, $everSponsor_i$)\n","2. Edge formation: Let $L_{ij,t}$ be an indicator for whether node i chose node j as a sponsor in transaction t.\n","3. Payoff structure:\n","$U_{ij,t}$ = $U^*(x_i, x_j; s_{i,t}, s_{jt}) + σε_{ij,t}$\n","    Where:\n","    - $s_{it}$, $s_{jt}$ are node-specific network statistics (e.g., degree centrality)\n","\n","4. Aggregate state variables:\n","    - $H^*(x; s)$: Inclusive value function, which is a fixed point condition we subject the log-lieklihood to inorder to avoid double counting of non zero links in the network.\n","    - $H^*(x_i, s_{it}) = \\frac{1}{n}∑_{k=1}^{n}Ψ_k(\\theta, H^*(x; s))$\n","    - $w_i = \\frac{\\mathbb{1}[s_{i,1} > 0]}{\\frac{1}{n}∑_{k=1}^{n}\\mathbb{1}(s_{k,1}>0) } $\n","    - **Psi function**: $ Ψ_i (\\theta,H) = w_i s_{i,1}* \\frac{exp[V(x, x_i, \\theta)]}{1+H(x_i)}$\n","    - $M^*(x_i, x_j, s_{it})$: The degree distribution of the network\n","    - $M^*(x_i, x_j, s_{it}) = \\frac{H*(x, s)}{1+H^*(x, s+1)}$\n","    \n","5. Limiting link frequency probability:\n","$$μ_0(L_{ij,t} = 1, s_{it}, s_{jt} | x_i, x_j) = \\frac{s_{it} s_{ij,t} *exp(U^*(x_i,x_j;s_{it}, s_{jt}) + U^*(x_j,x_i;s_{jt},s_{it}))}{(1 + H^*(x_i,s_{it}))*(1 + H^*(x_j,s_{jt}))}$$\n","\n","$$log{μ_0(L_{ij,t} = 1, s_{it}, s_{jt} | x_i, x_j)} = log{{s_{it}} + log{s_{ij,t}} + U^*(x_i,x_j;s_{it}, s_{jt}) + U^*(x_j,x_i;s_{jt},s_{it}) - log{1 + H^*(x_i,s_{it})} - log{1 + H^*(x_j,s_{jt})}$$\n","\n","6. Committee voting: Let $v_{kt}$ be the vote (white ball or black ball) of committee member k in transaction t. $$P(v_{kt} = white | x_i, x_{j1}, x_{j2}, x_k) = Φ(αx_i + β_1x_{j1} + β_2x_{j2} + γx_k + δ_1d(Z_i, Z_k) + δ_2d(Z_{j1}, Z_k) + δ_3d(Z_{j2}, Z_k))$$\n","Where:\n","    - Φ is the standard normal CDF\n","    - $Z_i$, $Z_{j1}$, $Z_{j2}$, $Z_k$ are latent positions in a low-dimensional Euclidean space\n","    d(·,·) is a distance function in this space.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCSalw6CCS0y","ExecuteTime":{"end_time":"2024-10-25T22:52:21.314704Z","start_time":"2024-10-25T22:52:21.276448Z"},"colab":{"base_uri":"https://localhost:8080/","height":350},"executionInfo":{"status":"error","timestamp":1730729447431,"user_tz":300,"elapsed":871,"user":{"displayName":"Xinchen Zhang","userId":"00644668196071921614"}},"outputId":"2232d170-d162-4ceb-be6a-0c0e6b080ff4"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'data/nyse_node_sp1.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-5b72f8a55a7a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read the data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m node_data = pd.read_csv('data/nyse_node_sp1.csv', header=None,\n\u001b[0m\u001b[1;32m      3\u001b[0m                         names=['name', 'ever_committee', 'node_id', 'ethnicity', 'ever_sponsor'])\n\u001b[1;32m      4\u001b[0m edge_data = pd.read_csv('data/nyse_edge_buy_sp_sp1.csv', header=None,\n\u001b[1;32m      5\u001b[0m                         names=['buyer_id', 'sponsor1_id', 'sponsor2_id', 'f1', 'f2', 'f3', 'f4', 'blackballs', 'whiteballs', 'year'])\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/nyse_node_sp1.csv'"]}],"source":["# Read the data files\n","node_data = pd.read_csv('data/nyse_node_sp1.csv', header=None,\n","                        names=['name', 'ever_committee', 'node_id', 'ethnicity', 'ever_sponsor'])\n","edge_data = pd.read_csv('data/nyse_edge_buy_sp_sp1.csv', header=None,\n","                        names=['buyer_id', 'sponsor1_id', 'sponsor2_id', 'f1', 'f2', 'f3', 'f4', 'blackballs', 'whiteballs', 'year'])\n","committee_data = pd.read_csv('data/nyse_edge_buy_com1.csv', header=None,\n","                             names=['buyer_id', 'committee_id', 'f1', 'f2', 'f3', 'f4', 'blackballs', 'whiteballs', 'year'])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EIUou9MUCS0y","ExecuteTime":{"end_time":"2024-10-25T22:52:22.409130Z","start_time":"2024-10-25T22:52:21.320855Z"}},"outputs":[],"source":["def process_data(node_data, edge_data, committee_data):\n","    node_data = pd.get_dummies(data=node_data, columns=['ethnicity'], dummy_na=True, prefix='ethnicity', drop_first=True, dtype=int)\n","    node_data[['ever_committee', 'ever_sponsor']] = node_data[['ever_committee', 'ever_sponsor']].fillna(0)\n","    node_attrs = node_data.set_index('node_id').drop(columns=['name']).T.to_dict('list')\n","\n","    # Initialize network statistics\n","    network_stats = {node_id: {'degree': 0, 'sponsor_count': 0} for node_id in node_attrs}\n","    edges = defaultdict(set)\n","\n","    transactions = []\n","    for _, row in edge_data.iterrows():\n","        buyer_id = row['buyer_id']\n","        sponsor1_id = row['sponsor1_id']\n","        sponsor2_id = row['sponsor2_id']\n","        year = row['year']\n","\n","        # Update network statistics\n","        network_stats[buyer_id]['degree'] += 2\n","        network_stats[sponsor1_id]['degree'] += 1\n","        network_stats[sponsor2_id]['degree'] += 1\n","        network_stats[sponsor1_id]['sponsor_count'] += 1\n","        network_stats[sponsor2_id]['sponsor_count'] += 1\n","        edges[buyer_id].add(sponsor1_id)\n","        edges[buyer_id].add(sponsor2_id)\n","\n","        committee_members = committee_data[(committee_data['buyer_id'] == buyer_id) &\n","                                           (committee_data['year'] == year)]['committee_id'].tolist()\n","\n","        transactions.append({\n","            'buyer_id': buyer_id,\n","            'sponsor1_id': sponsor1_id,\n","            'sponsor2_id': sponsor2_id,\n","            'committee_members': committee_members,\n","            'year': year,\n","            'whiteballs': row['whiteballs'],\n","            'blackballs': row['blackballs']\n","        })\n","\n","    return node_attrs, transactions, network_stats, edges\n","\n","node_attrs, transactions, network_stats, edges = process_data(node_data, edge_data, committee_data)\n","node_stats = {k: v['degree'] for k, v in network_stats.items()}"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"ncwCcLbsCS0y"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"id":"mNXoJSqlsAFE"},"source":["**Importance weights**\n","$$ w_i = \\frac{\\mathbb{1}[s_{i,1} > 0]}{\\frac{1}{n}∑_{k=1}^{n}\\mathbb{1}(s_{k,1}>0) } $$\n","**Psi function**\n","$$ Ψ_i (\\theta,H) = w_i s_{i,1}* \\frac{exp[V(x, x_i, \\theta)]}{1+H(x_i)}$$\n","\n","**Inclusive Value Function**: This is a fixed point condition we subject the log-lieklihood to inorder to avoid double counting of non zero links in the network.\n","$$ H(x) = \\frac{1}{n}∑_{i=1}^{n}\\psi_i(\\theta,H)$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aybSO8jRCS0y","ExecuteTime":{"end_time":"2024-10-25T22:52:22.415726Z","start_time":"2024-10-25T22:52:22.406624Z"}},"outputs":[],"source":["\n","class CachedVStar:\n","    def __init__(self, maxsize=10000):\n","        self.cache = {}\n","        self.maxsize = maxsize\n","\n","    def _create_symmetric_key(self, xi, xj, si, sj, theta):\n","        # Create two possible arrangements\n","        t = list(map(str, theta))\n","        key1 = '_'.join(list(map(str, xi)) + list(map(str, xj)) + [str(si), str(sj)] + t)\n","        key2 = '_'.join(list(map(str, xj)) + list(map(str, xi)) + [str(sj), str(si)] + t)\n","\n","        # Return the smaller key to ensure consistency\n","        return min(key1, key2)\n","\n","    @tf.function#(reduce_retracing=True)\n","    def U_star(self, xi, xj, si, sj, theta):\n","        # Define utility function\n","        input = tf.convert_to_tensor(list(xi) + list(xj) + [si] + [sj], dtype=tf.float32)\n","        theta = tf.convert_to_tensor(theta, dtype=tf.float32)\n","        return tf.tensordot(theta, input, axes=1)\n","\n","    # Define pseudo-surplus function (section 4.2 pg 34)\n","    @tf.function#(reduce_retracing=True)\n","    def V_star(self, xi, xj, si, sj, theta):\n","        return self.U_star(xi, xj, si, sj, theta) + self.U_star(xj, xi, sj, si, theta)\n","\n","    def __call__(self, xi, xj, si, sj, theta):\n","        key = self._create_symmetric_key(xi, xj, si, sj, theta)\n","        if key not in self.cache:\n","            if len(self.cache) >= self.maxsize:\n","                # Remove oldest entry\n","                self.cache.pop(next(iter(self.cache)))\n","            self.cache[key] = self.V_star(xi, xj, si, sj, theta)\n","        return self.cache[key]\n","\n","# Defined the importance weight function (section 4.2.1 pg 36)\n","@lru_cache(maxsize=10)\n","def calculate_weights(s, node_vals):\n","    t = tf.convert_to_tensor(list(node_vals))\n","    indicator_tensor = tf.cast(tf.greater(t, 0), tf.float32)\n","    denominator = tf.reduce_mean(indicator_tensor).numpy()\n","    return s / denominator\n","\n","# Define Psi function (section 4.2.1 pg 36)\n","@tf.function\n","def psi_i( H, x_i, s_i1, w_i, V):\n","    num = tf.exp(V)\n","    denom = 1 + H(x_i)\n","    res = w_i * s_i1 * num / denom\n","    return res\n","\n","# define the inclusive value function\n","@tf.function\n","def H_star(node_attrs, node_stats, weights, theta, H, v_star, max_iter=10):\n","\n","    # Initialize H(x) with zeros\n","    H_current = H\n","\n","    for i in tqdm(range(max_iter), desc='H_star', position=0, leave=True):\n","        H_prev = H_current\n","\n","        psi_mean = defaultdict(list)\n","        unique_node_pairs = set((tuple(node_attrs[node_id]), node_stats[node_id])\n","                           for node_id in node_attrs.keys())\n","        for x, s in tqdm(unique_node_pairs, desc='processing unique (x,s) pairs', position=1, leave=True):\n","            psi_values = []\n","            for i in node_attrs.keys():\n","                x_i, s_i = node_attrs[i], node_stats[i]\n","                V = v_star(x, x_i, s, s_i, theta)\n","                psi_i_value = psi_i( H_prev, x_i, s_i, weights[i], V)\n","                psi_values.append(psi_i_value)\n","\n","            psi_mean[str(x)].extend(psi_values)\n","        H_current = lambda x: tf.reduce_mean(tf.stack(psi_mean[str(x)]), axis=0)\n","\n","        # Check convergence - now comparing full tensors\n","        diff = tf.reduce_max(tf.abs(H_current(x) - H_prev(x)))\n","        if diff < 1e-3:\n","            print(f\"Convergence achieved after {i} iterations.\")\n","            break\n","\n","    return H_current\n","\n","# Log likelihood contribution for node_id\n","def log_likelihood_contribution(Lijt, x_i, x_j, s_i, s_j, theta, H, v_star):\n","    V = v_star(x_i, x_j, s_i, s_j, theta)\n","    ll = 0.5 * Lijt * (V - tf.math.log(1+ H(x_i)) - tf.math.log(1 + H(x_j)))\n","    return ll\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zX3pjy3TCS0y","ExecuteTime":{"end_time":"2024-10-25T22:52:22.428252Z","start_time":"2024-10-25T22:52:22.417836Z"}},"outputs":[],"source":["# Log limiting likelihood function\n","def log_likelihood(theta, node_attrs, node_stats, edges, weights, H = None):\n","\n","    # Initialize H if not given and loglikelihood\n","    print('Cache Vstar')\n","    cached_v_star = CachedVStar()\n","    H = H if H is not None else lambda x: tf.zeros_like(x)\n","    H = H_star(node_attrs, node_stats, weights, theta, H, cached_v_star)\n","    print('Calculated H')\n","    l_list = []\n","    ll = 0.0\n","    print('Calculating log likelihood')\n","    for i in tqdm(node_attrs.keys(), desc='Log likelihood', position=0, leave=True):\n","        x_i, s_i = node_attrs[i], node_stats[i]\n","        for j in edges[i]:\n","            if i == j:\n","                continue\n","            x_j, s_j = node_attrs[j], node_stats[j]\n","            ll += log_likelihood_contribution(tf.constant(1.0), x_i, x_j, s_i, s_j, theta, H, cached_v_star)\n","        ll += tf.math.log(float(s_i)) - tf.math.log(1 + H(x_i))\n","        l_list.append(ll)\n","    return ll, H, l_list\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"GP1cORJvCS0y"},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2i47r_1CS0z","ExecuteTime":{"end_time":"2024-10-25T22:52:22.475154Z","start_time":"2024-10-25T22:52:22.466148Z"}},"outputs":[],"source":["# Set up HMC\n","num_results = 1000\n","num_burnin_steps = 1000\n","\n","# Initialize parameters\n","x = len(list(node_attrs.values())[0])\n","initial_state = [0]*x*2 + [0, 0]  #x_i, x_j , si, sj\n","\n","# initial_state = tf.Variable(initial_state)\n","\n","# Define the HMC transition kernel\n","step_size = tf.Variable(0.01)\n","adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n","    tfp.mcmc.HamiltonianMonteCarlo(\n","        target_log_prob_fn=log_likelihood,\n","        num_leapfrog_steps=10,\n","        step_size=step_size),\n","    num_adaptation_steps=int(num_burnin_steps * 0.8))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZW_wnTs6Wgj","ExecuteTime":{"start_time":"2024-10-25T22:52:22.477261Z"}},"outputs":[],"source":["weights = {node_id: calculate_weights(s > 0, node_stats.values()) for node_id, s in node_stats.items() }\n","\n","ll, H, l_list = log_likelihood(initial_state, node_attrs, node_stats, edges, weights)\n","ll"]},{"cell_type":"code","outputs":[],"source":["import seaborn as sns\n","sns.lineplot(data=l_list)"],"metadata":{"id":"_t8P9vOa47aO"},"execution_count":null},{"cell_type":"code","outputs":[],"source":[],"metadata":{"id":"t_hdz7AT47aO"},"execution_count":null}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}